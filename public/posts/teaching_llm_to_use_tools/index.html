<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>AI 的未来，不在大脑，而在手：从 LLM 到工具驱动智能 | Thinking in Agents</title>
<meta name="keywords" content="">
<meta name="description" content="在 AI 领域，我们常陷入一个误区：认为智能的未来在于构建更大、更全知、更像人脑的模型，让它记住所有知识。然而，作为开发者，我们深知这种“记忆力”的上限和局限。人类的真正智能，并非依赖于单纯的知识存储，而在于解决问题的能力。
思考我们自己解决一个复杂数学题的流程：

分析问题：理解需求。
选择工具：判断需要哪些数学工具、公式或软件。
建模与执行：运用工具进行计算、推导。
验证与决策：检查结果，修正思路，得出最终结论。

在这个过程中，最核心的不是你“背诵”了多少公式，而是你知道何时何地调用何种工具，并能高效利用它们。这是一种以“结构化思维 &#43; 目标导向”为核心的模式。
LLM 的本质与局限：语言的概率压缩器
从开发者角度看，当前的大型语言模型（LLM）本质是一个强大的语言概率压缩器。它在海量文本数据中学习词语间的统计关联，从而能生成极为流利、上下文相关的文本。对于纯语言任务，其表现令人惊叹。
然而，当我们尝试让 LLM 驾驭非语言、强逻辑、精确性的领域时，如：

抽象符号运算（数学、物理）
精确定义匹配（工程规范）
严格逻辑推导（编程、决策树）
可验证的计算过程（财务、科学实验）

我们会发现语言模型固有的“概率化”机制，与这些领域要求的“确定性”之间存在着先天鸿沟。指望 LLM 仅仅靠扩大规模来跨越这条鸿沟，就像期望一个语言学家通过背诵更多诗歌就能成为量子物理学家一样，这是不现实且低效的。所以，需要借助工具扩展 LLM 的能力。
一、扩展 LLM 能力：借用人类“使用工具”的智慧
人类的进步正是通过外部工具不断突破自身生理和认知极限：计算器扩展了计算力，笔记扩展了记忆，显微镜扩展了感知，电脑扩展了逻辑运算能力。我们的大脑并非用于无限计算，而是用于调度工具、规划步骤、制定策略和验证结果。
因此，作为开发者，我们不应止步于“训练更大模型”，而应专注于赋予 LLM “使用工具”的能力。当前 AI 领域已清晰地展现了这一趋势和技术价值：


解决 LLM 算术硬伤：集成计算器工具

问题：即使是参数巨大的 LLM，在两位数以上的精确计算上依然错误频发，因为其内部知识是概率模式，而非符号计算引擎。
解决方案：通过将外部计算器作为工具接入。LLM 识别出计算任务后，不再尝试内部计算，而是将数字和运算符传递给计算器工具执行，再将准确结果整合到回复中。
价值：外部计算器提供了 LLM 内部无法比拟的精确执行能力，极大地提升了模型在数值任务上的可靠性。



克服 LLM 信息“幻觉”：引入搜索引擎与知识库工具

问题：LLM 训练数据的时效性、全面性限制，以及生成文本时为了流畅性可能出现的“编造”现象，导致其信息准确性难以保证。
解决方案：引入搜索引擎或外部知识库查询工具。LLM 在生成答案前，先调用这些工具检索最新、最准确的事实依据。例如，回答“最新诺贝尔奖得主”时，模型通过工具获取实时数据。
价值：外部工具弥补了 LLM 内部知识的盲区与时效性问题，提供了“事实校准”和“实时更新”的能力。



突破 LLM 逻辑推理瓶颈：集成代码解释器与符号系统

问题：LLM 在处理多步复杂逻辑推理、编程调试或科学问题时，往往难以保持推理链条的连贯和正确。
解决方案：

代码解释器（Code Interpreter）：LLM 生成 Python 等代码，然后将代码交给外部解释器运行验证。模型可根据运行结果（包括错误信息）进行调试和迭代。
符号推理系统：对于严谨的数学或逻辑问题，LLM 将问题转化为符号表达式，交由专门的工具处理。


价值：这些工具为 LLM 提供了其内部缺乏的精确执行和验证逻辑的能力，是其逻辑推理的强大“外挂”。



这些实践案例清晰地表明，在 AI 发展的关键阶段，尤其是面对需要超越语言模式、进行精确操作、获取实时信息和严格逻辑验证的场景时，仅仅依赖模型内部的“知识”是远远不够的。">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/teaching_llm_to_use_tools/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/teaching_llm_to_use_tools/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Thinking in Agents (Alt + H)">Thinking in Agents</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      AI 的未来，不在大脑，而在手：从 LLM 到工具驱动智能
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-11-20 13:59:13 +0800 CST'>November 20, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>在 AI 领域，我们常陷入一个误区：认为智能的未来在于构建<strong>更大、更全知、更像人脑</strong>的模型，让它记住所有知识。然而，作为开发者，我们深知这种“记忆力”的上限和局限。人类的真正智能，并非依赖于单纯的知识存储，而在于<strong>解决问题的能力</strong>。</p>
<p>思考我们自己解决一个复杂数学题的流程：</p>
<ol>
<li><strong>分析问题</strong>：理解需求。</li>
<li><strong>选择工具</strong>：判断需要哪些数学工具、公式或软件。</li>
<li><strong>建模与执行</strong>：运用工具进行计算、推导。</li>
<li><strong>验证与决策</strong>：检查结果，修正思路，得出最终结论。</li>
</ol>
<p>在这个过程中，最核心的不是你“背诵”了多少公式，而是你<strong>知道何时何地调用何种工具，并能高效利用它们</strong>。这是一种以“结构化思维 + 目标导向”为核心的模式。</p>
<h3 id="llm-的本质与局限语言的概率压缩器">LLM 的本质与局限：语言的概率压缩器<a hidden class="anchor" aria-hidden="true" href="#llm-的本质与局限语言的概率压缩器">#</a></h3>
<p>从开发者角度看，当前的大型语言模型（LLM）本质是一个<strong>强大的语言概率压缩器</strong>。它在海量文本数据中学习词语间的统计关联，从而能生成极为流利、上下文相关的文本。对于纯语言任务，其表现令人惊叹。</p>
<p>然而，当我们尝试让 LLM 驾驭<strong>非语言、强逻辑、精确性</strong>的领域时，如：</p>
<ul>
<li><strong>抽象符号运算</strong>（数学、物理）</li>
<li><strong>精确定义匹配</strong>（工程规范）</li>
<li><strong>严格逻辑推导</strong>（编程、决策树）</li>
<li><strong>可验证的计算过程</strong>（财务、科学实验）</li>
</ul>
<p>我们会发现语言模型固有的“概率化”机制，与这些领域要求的“确定性”之间存在着<strong>先天鸿沟</strong>。指望 LLM 仅仅靠扩大规模来跨越这条鸿沟，就像期望一个语言学家通过背诵更多诗歌就能成为量子物理学家一样，这是不现实且低效的。所以，需要借助工具扩展 LLM 的能力。</p>
<h2 id="一扩展-llm-能力借用人类使用工具的智慧">一、扩展 LLM 能力：借用人类“使用工具”的智慧<a hidden class="anchor" aria-hidden="true" href="#一扩展-llm-能力借用人类使用工具的智慧">#</a></h2>
<p>人类的进步正是通过外部工具不断<strong>突破自身生理和认知极限</strong>：计算器扩展了计算力，笔记扩展了记忆，显微镜扩展了感知，电脑扩展了逻辑运算能力。我们的大脑并非用于无限计算，而是用于<strong>调度工具、规划步骤、制定策略和验证结果。</strong></p>
<p>因此，作为开发者，我们不应止步于“训练更大模型”，而应专注于赋予 LLM <strong>“使用工具”的能力</strong>。当前 AI 领域已清晰地展现了这一趋势和技术价值：</p>
<ol>
<li>
<p><strong>解决 LLM 算术硬伤：集成计算器工具</strong></p>
<ul>
<li><strong>问题</strong>：即使是参数巨大的 LLM，在两位数以上的精确计算上依然错误频发，因为其内部知识是概率模式，而非符号计算引擎。</li>
<li><strong>解决方案</strong>：通过<strong>将外部计算器作为工具</strong>接入。LLM 识别出计算任务后，不再尝试内部计算，而是将数字和运算符传递给计算器工具执行，再将准确结果整合到回复中。</li>
<li><strong>价值</strong>：外部计算器提供了 LLM 内部无法比拟的精确执行能力，极大地提升了模型在数值任务上的可靠性。</li>
</ul>
</li>
<li>
<p><strong>克服 LLM 信息“幻觉”：引入搜索引擎与知识库工具</strong></p>
<ul>
<li><strong>问题</strong>：LLM 训练数据的时效性、全面性限制，以及生成文本时为了流畅性可能出现的“编造”现象，导致其信息准确性难以保证。</li>
<li><strong>解决方案</strong>：引入<strong>搜索引擎或外部知识库查询工具</strong>。LLM 在生成答案前，先调用这些工具检索最新、最准确的事实依据。例如，回答“最新诺贝尔奖得主”时，模型通过工具获取实时数据。</li>
<li><strong>价值</strong>：外部工具弥补了 LLM 内部知识的盲区与时效性问题，提供了“事实校准”和“实时更新”的能力。</li>
</ul>
</li>
<li>
<p><strong>突破 LLM 逻辑推理瓶颈：集成代码解释器与符号系统</strong></p>
<ul>
<li><strong>问题</strong>：LLM 在处理多步复杂逻辑推理、编程调试或科学问题时，往往难以保持推理链条的连贯和正确。</li>
<li><strong>解决方案</strong>：
<ul>
<li><strong>代码解释器（Code Interpreter）</strong>：LLM 生成 Python 等代码，然后将代码交给外部解释器运行验证。模型可根据运行结果（包括错误信息）进行调试和迭代。</li>
<li><strong>符号推理系统</strong>：对于严谨的数学或逻辑问题，LLM 将问题转化为符号表达式，交由专门的工具处理。</li>
</ul>
</li>
<li><strong>价值</strong>：这些工具为 LLM 提供了其内部缺乏的精确执行和验证逻辑的能力，是其逻辑推理的强大“外挂”。</li>
</ul>
</li>
</ol>
<p>这些实践案例清晰地表明，在 AI 发展的关键阶段，尤其是面对需要<strong>超越语言模式、进行精确操作、获取实时信息和严格逻辑验证</strong>的场景时，<strong>仅仅依赖模型内部的“知识”是远远不够的。</strong></p>
<p>作为开发者，我们的任务是：将大模型定位为<strong>决策和调度“大脑”</strong>，同时为其配备可操作的<strong>外部工具作为其“手”</strong>。这是构建更强大、更可靠、更通用、真正能解决实际问题的智能系统的关键路径。</p>
<p>那么，这种“大脑”与“手”的协作，在技术层面是如何实现的？AI 如何才能真正学会使用工具，甚至像一个团队一样高效协作？</p>
<h2 id="二功能调用function-call">二、功能调用（Function Call）<a hidden class="anchor" aria-hidden="true" href="#二功能调用function-call">#</a></h2>
<p><strong>功能调用</strong>，也称为函数调用,是一种允许 LLM 与外部系统、API 和工具交互的技术。 通过为 LLM 提供一组函数或工具及其描述和使用说明，该模型可以智能地选择和调用适当的函数来完成给定的任务。它让 LLM 获得了一种全新的能力：从仅仅“生成文本”跃升到“执行行动”。其工作原理如下：</p>
<ol>
<li><strong>工具注册</strong>：开发者向 LLM 提供一组可用的工具清单。每个工具都有清晰的<strong>名称、描述</strong>（LLM 可以理解的自然语言说明其功能）和<strong>参数规范</strong>（例如，一个计算器工具可能需要 <code>num1</code>、<code>num2</code>、<code>operation</code> 等参数，并说明这些参数的数据类型和含义）。</li>
<li><strong>意图识别与参数提取</strong>：当 LLM 接收到用户输入或在多步推理中需要外部信息时，它会结合其强大的语言理解能力，识别出用户或任务的真实意图。如果这个意图需要工具才能实现，LLM 会：选择最匹配的工具，并从用户输入或自身上下文推理中提取出调用该工具所需的所有参数。</li>
<li><strong>结构化调用生成</strong>：LLM 不再直接生成一个自然语言答案，而是生成一个<strong>结构化的 JSON 对象或类似格式的文本</strong>，清晰地指定了要调用的工具名称及其对应的参数值。</li>
<li><strong>外部执行与结果返回</strong>：这个结构化的调用指令会被传递给外部执行器。执行器调用真正的工具（例如，一个 Python 函数、一个 API 接口），获取工具执行的结果。</li>
<li><strong>结果整合与后续推理</strong>：工具执行的结果（例如，计算器的结果、数据库查询的数据、API 的返回信息）会再次送回 LLM。LLM 利用这些新的信息进行后续的推理、总结，并生成最终的用户回复。</li>
</ol>
<p><img alt="this is a image" loading="lazy" src="/images/llm_function_call.jpeg"></p>
<p><strong>基于 LangChain 的功能调用示例：</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.tools <span style="color:#f92672">import</span> tool
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.agents <span style="color:#f92672">import</span> AgentExecutor, create_tool_calling_agent
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 假设你已经设置了 OPENAI_API_KEY</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;your_openai_key&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 定义工具 (Tools)</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Adds two integers and returns the result.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiplies two integers and returns the result.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 准备一个 LLM</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> ChatOpenAI(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-4o&#34;</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># 选用支持function call的模型</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 创建一个 Prompt Template</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LangChain 会自动将工具信息注入到这个prompt中</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;You are a helpful assistant. Use the available tools to answer questions.&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;placeholder&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{agent_scratchpad}</span><span style="color:#e6db74">&#34;</span>), <span style="color:#75715e"># 这是Agent思考和工具调用的“草稿板”</span>
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. 将工具和 LLM 组装成一个 Agent</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># create_tool_calling_agent 会自动处理LLM的function call输出</span>
</span></span><span style="display:flex;"><span>tools <span style="color:#f92672">=</span> [add, multiply]
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> create_tool_calling_agent(llm, tools, prompt)
</span></span><span style="display:flex;"><span>agent_executor <span style="color:#f92672">=</span> AgentExecutor(agent<span style="color:#f92672">=</span>agent, tools<span style="color:#f92672">=</span>tools, verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. 用户输入与执行</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;--- 第一次调用：计算 ---&#34;</span>)
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> agent_executor<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;What is 123 + 456?&#34;</span>})
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Agent&#39;s final response: </span><span style="color:#e6db74">{</span>response[<span style="color:#e6db74">&#39;output&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">--- 第二次调用：乘法 ---&#34;</span>)
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> agent_executor<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;And what about 7 times 8?&#34;</span>})
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Agent&#39;s final response: </span><span style="color:#e6db74">{</span>response[<span style="color:#e6db74">&#39;output&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 内部机制简述：</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LLM 接收到“What is 123 + 456?”后，会根据 prompt 中对工具的描述，</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 判断需要调用 `add` 工具，并提取参数 `a=123`, `b=456`。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LLM 生成一个类似 {&#34;tool_name&#34;: &#34;add&#34;, &#34;args&#34;: {&#34;a&#34;: 123, &#34;b&#34;: 456}} 的结构化调用请求。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># AgentExecutor 捕捉到这个请求，实际执行 `add(123, 456)`，得到结果 `579`。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># `579` 作为工具的观察结果，被再次送回 LLM。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LLM 整合这个结果，生成最终的自然语言回复：“123 + 456 等于 579。”</span>
</span></span></code></pre></div><p><strong>这是一种将“语言理解”与“外部能力执行”解耦的方法，使模型能够主动使用外部工具。</strong>
功能调用的核心价值在于：</p>
<ul>
<li><strong>职责分离</strong>：LLM 专注于理解、规划和最终的语言生成，而具体的执行逻辑则委托给专业的外部工具，确保了执行的<strong>准确性和可靠性</strong>。</li>
<li><strong>能力拓展</strong>：赋予 LLM 主动识别需求、调用工具解决问题的能力，极大地拓宽了其应用边界，使其能处理远超自身语言生成范围的任务。</li>
<li><strong>可控性与安全性</strong>：通过结构化调用，可以避免 LLM 直接生成随意代码或不规范的 API 请求。外部执行器能进行参数校验、错误处理，并确保调用过程的安全和规范。</li>
<li><strong>可审计性</strong>：每次工具调用都留下明确的执行记录，便于调试、优化和监控整个智能系统的运行。</li>
</ul>
<p>功能调用的出现，无疑是 LLM 能力扩展的一个里程碑，它首次赋予了大型语言模型“调用外部能力”的 “手”。然而，作为一名开发者，在实际构建和部署复杂 AI 应用时，我们很快会发现功能调用的局限性。随着应用规模扩大、工具数量增长，以及对系统鲁棒性和可控性要求的提升，这些限制变得日益突出：</p>
<h3 id="1-单步原子操作与复杂任务流的鸿沟"><strong>1. 单步原子操作与复杂任务流的鸿沟</strong><a hidden class="anchor" aria-hidden="true" href="#1-单步原子操作与复杂任务流的鸿沟">#</a></h3>
<p>功能调用的核心机制是：LLM 根据上下文生成一个结构化的工具调用请求，然后外部执行器执行该工具并返回结果。这本质上是一种<strong>原子化的“感知-行动”循环</strong>：</p>
<blockquote>
<p>LLM → [调用某个工具 (带参数)] → [工具执行并返回结果] → LLM 接收并继续。</p></blockquote>
<p>然而，真实的智能系统往往需要处理：</p>
<ul>
<li><strong>多工具组合与链式调用</strong>：例如，先查询数据库，再用计算器处理查询结果，最后用绘图工具可视化。</li>
<li><strong>跨步骤的上下文依赖与共享</strong>：前一步工具的输出如何结构化地成为下一步工具的输入，或影响后续决策？</li>
<li><strong>动态规划与条件分支</strong>：根据工具返回结果，智能地决定下一步是调用 A 工具还是 B 工具，甚至进行错误恢复。</li>
<li><strong>长周期任务状态管理</strong>：如何在多轮对话或长时间运行的任务中，保持对全局状态的理解和一致性？</li>
</ul>
<p>功能调用本身不提供这些高级机制来<strong>编排复杂任务链</strong>。开发者需要自行在 LLM 外部构建复杂的 Agent 逻辑、状态机或编排引擎来管理这些流程，徒增系统复杂度。</p>
<h3 id="2-缺乏统一的工具协议与互操作标准"><strong>2. 缺乏统一的工具协议与互操作标准</strong><a hidden class="anchor" aria-hidden="true" href="#2-缺乏统一的工具协议与互操作标准">#</a></h3>
<p>当前工具生态中，每个工具都可能拥有自己独立的：</p>
<ul>
<li><strong>输入 Schema 定义</strong>：参数类型、必填项、可选项的描述方式各异。</li>
<li><strong>输出格式</strong>：返回 JSON、字符串、列表等，且结构不统一。</li>
<li><strong>元数据</strong>：name、description 仅仅是自然语言描述，缺乏机器可读的标准化协议。</li>
<li><strong>调用方式</strong>：可能直接是 Python 函数，也可能是 RESTful API，或是 RPC 调用。</li>
</ul>
<p>这种碎片化导致：</p>
<ul>
<li><strong>工具发现与理解困难</strong>：新工具集成时，LLM 难以凭空“理解”其精确功能和使用方式，需要大量的 Prompt Engineering 或人工标注。</li>
<li><strong>组合与互操作障碍</strong>：不同工具之间的数据格式不兼容，难以无缝组合。</li>
<li><strong>开发与维护成本高昂</strong>：开发者需要针对每个工具进行适配，难以遵守统一规范。</li>
</ul>
<p>简而言之，功能调用提供了 LLM 调用外部工具的“接口能力”，但整个工具生态系统缺乏一套统一的“通信协议”。</p>
<h3 id="3-缺乏系统级的安全权限与资源管理"><strong>3. 缺乏系统级的安全、权限与资源管理</strong><a hidden class="anchor" aria-hidden="true" href="#3-缺乏系统级的安全权限与资源管理">#</a></h3>
<p>在企业级或生产环境中，我们不能简单地将工具暴露给 LLM 调用。功能调用机制在安全性、可控性方面存在天然的不足：</p>
<ul>
<li><strong>权限与鉴权缺失</strong>：调用一个工具是否需要特定的用户权限或身份认证？功能调用本身不提供这样的管理层。</li>
<li><strong>资源限制与配额</strong>：一个工具的调用是否会消耗大量计算资源或触发费用？如何限制 LLM 对某个高成本工具的无限调用？</li>
<li><strong>数据隔离与审计</strong>：工具调用过程中涉及的数据是否敏感？调用日志是否详细到足以进行审计和追踪？</li>
<li><strong>副作用管理</strong>：工具调用是否会修改外部环境（如写入数据库、发送邮件）？如何防止 LLM 在不当情境下触发这些高风险操作？</li>
</ul>
<p><strong>功能调用仅仅是“能力暴露”的机制，而非“能力管理和治理”的解决方案。</strong> 在生产环境上，这种缺乏系统级安全和治理的能力是无法接受的。</p>
<h3 id="4-难以表达与支持复杂的多智能体协作"><strong>4. 难以表达与支持复杂的多智能体协作</strong><a hidden class="anchor" aria-hidden="true" href="#4-难以表达与支持复杂的多智能体协作">#</a></h3>
<p>功能调用设计之初，其视角是：<strong>单个 LLM 作为核心大脑，直接调度工具。</strong> 然而，未来的智能系统将是<strong>多 Agent、多角色、异步协作</strong>的复杂生态：</p>
<ul>
<li><strong>任务分解与并行</strong>：一个复杂任务需要分解为多个子任务，由不同的专业 Agent 或工具并行处理。</li>
<li><strong>角色分工与专业化</strong>：一个 Agent 负责规划，另一个 Agent 负责执行，再一个 Agent 负责验证。</li>
<li><strong>异步通信与状态同步</strong>：Agent 之间需要非阻塞地通信，并保持对共享状态的一致性理解。</li>
</ul>
<p>功能调用的点对点调用模式，难以自然地表达和支持这种复杂的<strong>多 Agent 互操作结构和协同机制。</strong></p>
<h2 id="三模型上下文协议-mcp-model-context-protocol">三、模型上下文协议 (MCP, Model Context Protocol)<a hidden class="anchor" aria-hidden="true" href="#三模型上下文协议-mcp-model-context-protocol">#</a></h2>
<p><strong>模型上下文协议（MCP）</strong> 正是为了解决功能调用的上述深层局限而应运而生。2024 年 11 月底，由 Anthropic 推出的一种开放标准，旨在统一大模型与外部数据源和工具之间的通信协议。
其核心宗旨是为 AI 应用提供一个<strong>通用的接口</strong>，使其能够无缝地连接到各种外部系统。想象一下，如果 AI 应用拥有一个像 <strong>USB-C 端口</strong>一样的标准化接口——这就是 MCP 所要实现的目标。正如 USB-C 为电子设备提供了统一的连接方式，无论是充电、数据传输还是视频输出，都能通过这一个接口完成；MCP 则为 AI 应用提供了一个<strong>标准化的途径，使其能够接入并利用外部世界的能力和信息。</strong></p>
<p>通过 MCP，LLM 驱动的 AI 应用，能够极大地扩展其能力边界，从而：</p>
<ul>
<li><strong>连接到丰富的数据源</strong>：无论是本地文件系统、企业内部数据库、实时 API 数据流，还是互联网上的非结构化信息，MCP 都允许 AI 应用以结构化和可控的方式获取所需数据。</li>
<li><strong>调用强大的外部工具</strong>：AI 不再局限于其内在知识，而是能像人类使用工具一样，调用搜索引擎进行实时信息检索、利用计算器进行精确运算、或是通过专业的图像处理工具进行视觉内容创作。</li>
<li><strong>集成到复杂的工作流</strong>：MCP 使 AI 应用能够被编排到更宏大的自动化工作流中，例如触发特定的脚本、调用专业领域的 API 服务、或是根据上下文执行一系列预定义的指令，甚至可以是对其自身进行优化的“专业提示词”工作流。</li>
</ul>
<p><img alt="this is a image" loading="lazy" src="/images/model_context_protocol.jpeg"></p>
<h3 id="构建-mcp-服务的示例">构建 MCP 服务的示例<a hidden class="anchor" aria-hidden="true" href="#构建-mcp-服务的示例">#</a></h3>
<p>这里基于 FastMCP 演示如何定义工具、实现一个 MCP server，以及通过一个简单客户端进行调用,最后与 LLM 集成使用。</p>
<h4 id="1-安装依赖">1. 安装依赖<a hidden class="anchor" aria-hidden="true" href="#1-安装依赖">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install fastmcp
</span></span></code></pre></div><h4 id="2-实现-mcp-服务端server">2. 实现 MCP 服务端（Server）<a hidden class="anchor" aria-hidden="true" href="#2-实现-mcp-服务端server">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> fastmcp <span style="color:#f92672">import</span> FastMCP
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建一个 FastMCP 实例（MCP Server）</span>
</span></span><span style="display:flex;"><span>mcp <span style="color:#f92672">=</span> FastMCP(<span style="color:#e6db74">&#34;MyMCPServer&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义一个简单工具：加法</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@mcp.tool</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Add two numbers.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 再定义一个示例资源（Model Context Protocol 支持资源）</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@mcp.resource</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">version</span>() <span style="color:#f92672">-&gt;</span> dict:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Return server version info.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;version&#34;</span>: <span style="color:#e6db74">&#34;1.0.0&#34;</span>, <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;MyMCPServer&#34;</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 启动 MCP 服务（支持多种 transport 协议，比如 SSE / HTTP / STDIO）</span>
</span></span><span style="display:flex;"><span>    mcp<span style="color:#f92672">.</span>run(transport<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sse&#34;</span>, host<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;127.0.0.1&#34;</span>, port<span style="color:#f92672">=</span><span style="color:#ae81ff">8000</span>)
</span></span></code></pre></div><h4 id="3-实现-mcp-客户端client">3. 实现 MCP 客户端（Client）<a hidden class="anchor" aria-hidden="true" href="#3-实现-mcp-客户端client">#</a></h4>
<p>下面是一个简单异步客户端，调用上面定义的 add 工具：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> asyncio
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> fastmcp <span style="color:#f92672">import</span> Client, ClientTransport
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 连接到 MCP 服务</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">with</span> Client(<span style="color:#e6db74">&#34;http://127.0.0.1:8000/mcp&#34;</span>, transport<span style="color:#f92672">=</span>ClientTransport<span style="color:#f92672">.</span>HTTP) <span style="color:#66d9ef">as</span> client:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 调用 add 工具</span>
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> client<span style="color:#f92672">.</span>call_tool(
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;add&#34;</span>,
</span></span><span style="display:flex;"><span>            arguments<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">10</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">20</span>}
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Result of add:&#34;</span>, result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 获取资源（version）</span>
</span></span><span style="display:flex;"><span>        info <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> client<span style="color:#f92672">.</span>call_resource(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;version&#34;</span>, arguments<span style="color:#f92672">=</span>{})
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Server info:&#34;</span>, info)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    asyncio<span style="color:#f92672">.</span>run(main())
</span></span></code></pre></div><h2 id="4-将-llm-与-mcp-集成">4. 将 LLM 与 MCP 集成<a hidden class="anchor" aria-hidden="true" href="#4-将-llm-与-mcp-集成">#</a></h2>
<p>这里基于 LangChain 将 LLM 和 MCP 集成：：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> fastmcp <span style="color:#f92672">import</span> Client, ClientTransport
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> asyncio
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json <span style="color:#75715e"># 用于处理JSON格式的工具描述和LLM响应</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> ChatOpenAI(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;qwen-max&#34;</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">agent_with_mcp</span>(query: str):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 初始化 MCP 客户端</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">with</span> Client(<span style="color:#e6db74">&#34;http://127.00.1:8000/mcp&#34;</span>, transport<span style="color:#f92672">=</span>ClientTransport<span style="color:#f92672">.</span>HTTP) <span style="color:#66d9ef">as</span> client:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 1. 从 MCP 服务获取所有可用工具的列表和描述</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 假设 client.get_tools() 返回一个列表，每个元素是工具的 dict</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 例如：[{&#34;name&#34;: &#34;add&#34;, &#34;description&#34;: &#34;Adds two numbers&#34;, &#34;parameters&#34;: {&#34;type&#34;: &#34;object&#34;, &#34;properties&#34;: {&#34;a&#34;: {&#34;type&#34;: &#34;number&#34;}, &#34;b&#34;: {&#34;type&#34;: &#34;number&#34;}}}}, ...]</span>
</span></span><span style="display:flex;"><span>        available_tools <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> client<span style="color:#f92672">.</span>get_tools()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 2. 将工具信息格式化，注入到 LLM 的 prompt 中</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 这里使用一个简单的模板，实际应用中可以更复杂</span>
</span></span><span style="display:flex;"><span>        tool_descriptions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> tool <span style="color:#f92672">in</span> available_tools:
</span></span><span style="display:flex;"><span>            tool_descriptions<span style="color:#f92672">.</span>append(
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Tool Name: </span><span style="color:#e6db74">{</span>tool[<span style="color:#e6db74">&#39;name&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Description: </span><span style="color:#e6db74">{</span>tool[<span style="color:#e6db74">&#39;description&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Parameters: </span><span style="color:#e6db74">{</span>json<span style="color:#f92672">.</span>dumps(tool[<span style="color:#e6db74">&#39;parameters&#39;</span>])<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 构建一个明确指导LLM使用工具的Prompt</span>
</span></span><span style="display:flex;"><span>        system_prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;You are an AI assistant capable of using external tools. &#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;Here are the tools you can use:</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>join(tool_descriptions) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;Based on the user&#39;s request, decide if you need a tool. &#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;If so, respond with a JSON object in the format: &#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;{&#34;tool_name&#34;: &#34;...&#34;, &#34;tool_args&#34;: {...}} &#39;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;Otherwise, respond with a natural language answer.&#34;</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. LLM 根据用户查询和工具描述，判断是否需要调用工具</span>
</span></span><span style="display:flex;"><span>        llm_response_str <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> llm<span style="color:#f92672">.</span>agenerate(
</span></span><span style="display:flex;"><span>            [{<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: system_prompt},
</span></span><span style="display:flex;"><span>             {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: query}]
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 假设llm_response_str是LLM的原始输出字符串</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 需要解析LLM的响应，提取工具调用信息</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># LLM通常会输出一个包含JSON的字符串，需要提取JSON部分</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 实际解析可能需要更复杂的正则或LLM的结构化输出能力</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 假设LLM直接返回了 {&#34;tool_name&#34;: &#34;add&#34;, &#34;tool_args&#34;: {&#34;a&#34;: 10, &#34;b&#34;: 20}}</span>
</span></span><span style="display:flex;"><span>            llm_output <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(llm_response_str)
</span></span><span style="display:flex;"><span>            tool_name <span style="color:#f92672">=</span> llm_output<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;tool_name&#34;</span>)
</span></span><span style="display:flex;"><span>            tool_args <span style="color:#f92672">=</span> llm_output<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;tool_args&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> tool_name <span style="color:#f92672">and</span> tool_args:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;LLM decided to call tool: </span><span style="color:#e6db74">{</span>tool_name<span style="color:#e6db74">}</span><span style="color:#e6db74"> with args: </span><span style="color:#e6db74">{</span>tool_args<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># 4. 调用 MCP 服务中的工具</span>
</span></span><span style="display:flex;"><span>                resp <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> client<span style="color:#f92672">.</span>call_tool(
</span></span><span style="display:flex;"><span>                    name<span style="color:#f92672">=</span>tool_name,
</span></span><span style="display:flex;"><span>                    arguments<span style="color:#f92672">=</span>tool_args
</span></span><span style="display:flex;"><span>                )
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">return</span> resp
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> llm_response_str <span style="color:#75715e"># LLM没有选择工具，返回自然语言回复</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> json<span style="color:#f92672">.</span>JSONDecodeError:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> llm_response_str <span style="color:#75715e"># LLM返回的不是JSON，直接作为自然语言回复</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 运行示例</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 确保 fastmcp server 在 http://127.0.0.1:8000/mcp 运行，并注册了 &#34;add&#34; 工具</span>
</span></span><span style="display:flex;"><span>print(asyncio<span style="color:#f92672">.</span>run(agent_with_mcp(<span style="color:#e6db74">&#34;帮我算 10 加 20 等于多少？&#34;</span>)))
</span></span><span style="display:flex;"><span>print(asyncio<span style="color:#f92672">.</span>run(agent_with_mcp(<span style="color:#e6db74">&#34;你好，你是一个什么助手？&#34;</span>))) <span style="color:#75715e"># 测试不调用工具的场景</span>
</span></span></code></pre></div><h2 id="四langchain-中工具tools的管理与增强机制">四、LangChain 中工具（Tools）的管理与增强机制<a hidden class="anchor" aria-hidden="true" href="#四langchain-中工具tools的管理与增强机制">#</a></h2>
<p>除了像 MCP 那样定义一套开发标准解决功能调用的局限性，LangChain 从开发框架的视角，对功能调用进行了更高级的管理和增强机制。它是通过引入中间件（Middleware）的架构设计，将功能调用的原子性调用能力提升到更高级、更智能、更具工程化特性的管理层面。
在 LangChain 框架中，中间件的角色不是传统的简单拦截器，它被设计成为一种高级的、可感知上下文的“智能代理工具”。这些中间件的核心价值在于，它们能够对功能调用进行深层次的增强和复杂编排。
具体而言，LangChain 的中间件将那些原本需要开发者在 LLM 外部手动编写的复杂编排、状态管理和高级逻辑，巧妙地封装成可被 LLM 调用的“智能代理工具”。这意味着，LLM 不再是直接调用底层原子工具（例如一个简单的加法函数或单一的 API 请求）。相反，LLM 会通过标准的功能调用机制来调用这些“中间件工具”。而这些“中间件工具”在被调用后，会在其内部完成更复杂的、多步骤的、有状态的逻辑处理，甚至可能在内部进行多次底层工具调用或 LLM 推理，从而为 LLM 带来更强大的执行能力和更丰富的环境交互。</p>
<h3 id="langchain-中间件中-tools-的设计特点">LangChain 中间件中 Tools 的设计特点*<a hidden class="anchor" aria-hidden="true" href="#langchain-中间件中-tools-的设计特点">#</a></h3>
<ol>
<li>
<p><strong>AgentMiddleware 作为核心扩展机制</strong></p>
<ul>
<li>在 LangChain v1 中，<code>Middleware</code> 被设计为流程循环的核心扩展机制。它允许开发者在 Agent 循环的关键阶段（如 <code>before_model</code>、<code>modify_model_request</code>、<code>after_model</code>）插入自定义钩子（hook）。</li>
<li>这些中间件能够<strong>动态调整模型调用时的参数、可用的工具列表、甚至是提示词 (prompt)</strong>，极大地提升了 Agent 的灵活性和适应性。它们甚至可以决定是否“跳转”到 Agent 循环的不同节点，例如，决定是继续让 LLM 对话，还是触发一个复杂的工具调用序列。</li>
</ul>
</li>
<li>
<p><strong>工具（Tools）的动态注册与注入</strong></p>
<ul>
<li>与传统功能调用中工具列表通常是静态定义的不同，LangChain 的中间件允许<strong>动态注册和注入工具</strong>。这意味着工具并不需要预先全部在 Agent 初始化时给出。</li>
<li>例如，<code>TodoListMiddleware</code> 可以根据需要注入一个 <code>write_todos</code> 工具，而 <code>FileSearchMiddleware</code> 则会创建并管理 <code>glob</code> 和 <code>grep</code> 这类文件系统操作工具。这种动态性使得 Agent 能在不同场景下，只暴露和使用最相关的工具集。</li>
</ul>
</li>
<li>
<p><strong>内建的状态管理能力</strong></p>
<ul>
<li>这是 LangChain 中间件相较于传统功能调用的一个显著优势。中间件可以声明自己的 <code>state_schema</code>，并在每次模型调用之前、之后更新 Agent 的共享状态。这意味着中间件不仅能插入行为，还能<strong>拥有和管理自己的持久化状态</strong>。</li>
<li>例如，<code>TodoListMiddleware</code> 会维护一个“待办任务列表”的状态（包括任务的创建、完成等），而 <code>FileSearchMiddleware</code> 则会管理关于文件系统搜索的记录（如搜索路径、搜索结果等），这些状态都融入到对话的上下文中。这使得 Agent 能处理更复杂、多步骤且有记忆的任务。</li>
</ul>
</li>
<li>
<p><strong>对工具调用的深度拦截与封装</strong></p>
<ul>
<li>LangChain 的中间件可以在工具调用发生时（即功能调用触发时）进行拦截（wrap）。通过 <code>wrap_tool_call</code> 等钩子，中间件能够在工具实际执行前后插入各种逻辑，包括：
<ul>
<li><strong>日志记录</strong>：详细记录工具调用参数、耗时和结果。</li>
<li><strong>重试机制</strong>：在工具调用失败时自动进行重试。</li>
<li><strong>错误捕获与处理</strong>：捕获工具执行的异常，并以更友好的方式返回给 LLM 或尝试恢复。</li>
<li><strong>权限与安全控制</strong>：在工具执行前进行权限验证或资源配额检查。</li>
</ul>
</li>
<li>这种设计使得工具调用本身变得更加健壮和可控，将很多非功能性需求从 LLM 的推理负担中剥离出来，下沉到中间件层处理。</li>
</ul>
</li>
</ol>
<p><img alt="this is a image" loading="lazy" src="/images/langchain_todos_search_middleware.jpeg"></p>
<h3 id="优势与价值">优势与价值<a hidden class="anchor" aria-hidden="true" href="#优势与价值">#</a></h3>
<p>LangChain 的这种基于中间件的功能调用实现带来了显著的优势：</p>
<ul>
<li><strong>降低主 LLM 的认知负担</strong>：将复杂的任务编排、子任务规划、状态管理和低级错误处理从主 LLM 中剥离，转移到专业的中间件工具中。主 LLM 只需专注于更高层次的决策和与中间件工具的交互。</li>
<li><strong>实现复杂任务链的编排</strong>：中间件可以维护状态并执行多步逻辑，使得 LLM 能够处理需要多个工具协作、具有条件分支和长期记忆的复杂任务，而不仅仅是原子性的单步调用。</li>
<li><strong>提升系统鲁棒性与可控性</strong>：通过中间件的拦截机制，可以集中处理错误恢复、日志、性能监控、权限校验等工程化需求，使得 AI 系统更稳定、更安全。</li>
<li><strong>增强模块化与可重用性</strong>：将特定领域（如待办事项管理、文件系统操作）的复杂逻辑封装在可重用的中间件工具中，提高了代码的模块化程度和组件的复用性。</li>
<li><strong>动态适应性</strong>：中间件可以根据 AI 系统当前状态和任务需求，动态地注入或调整可用的工具集，使得系统更加灵活。</li>
</ul>
<h2 id="kimi-k2-通过后训练提升工具使用能力"><strong>Kimi K2 通过后训练提升工具使用能力</strong><a hidden class="anchor" aria-hidden="true" href="#kimi-k2-通过后训练提升工具使用能力">#</a></h2>
<p>模型上下文协议提供标准化基础，LangChain 的中间件机制如何通过上层封装来增强和管理工具的使用。这些都是在<strong>框架和系统层面</strong>解决 LLM 工具使用问题的方法。</p>
<p>然而，即便有了这些精巧的外部机制，一个核心问题依然存在：<strong>LLM 本身如何才能更智能地“知道何时该用工具、如何用工具”？</strong> 仅仅向模型提供工具的描述和接口（无论这些接口多么标准化，如 MCP），并不足以保证模型能高效、准确地做出工具使用决策。这就像是给了一个人一把瑞士军刀，但不教他如何在不同场景下使用每种工具。</p>
<p>Moonshot AI 的 Kimi K2 模型正是针对这一痛点，通过其独特的<strong>后训练（post-training）<strong>策略，从</strong>模型底层能力</strong>上强化了 LLM 的工具调用和 Agent 工作流能力。它不再仅仅依赖外部框架来编排，而是让模型自身内化了更强的工具使用策略和判断力。</p>
<h3 id="kimi-k2-的方法与流程工具使用能力的内化">Kimi K2 的方法与流程：工具使用能力的内化<a hidden class="anchor" aria-hidden="true" href="#kimi-k2-的方法与流程工具使用能力的内化">#</a></h3>
<p>Kimi K2 的后训练过程旨在让模型不仅拥有“工具”，更拥有“使用工具的智慧”，其核心流程如下：</p>
<ol>
<li>
<p><strong>大规模收集真实与合成工具（MCP/Tool 接口）</strong></p>
<ul>
<li>Moonshot AI 首先从 GitHub 等开源平台收集了 3,000 多个现有工具。这些工具被视为“工具空间”的基础，并且着重筛选那些具有良好接口规范（例如遵循类似 MCP 的设计原则）的工具。</li>
<li>这个步骤确保了模型能够接触到多样化、结构化的工具描述，是理解工具功能的前提。</li>
</ul>
</li>
<li>
<p><strong>生成海量“合成工具”</strong></p>
<ul>
<li>为了扩大训练数据的多样性和覆盖面，Moonshot AI 并没有止步于现有工具。他们通过对现有工具进行 embedding 聚类，然后生成了约 2 万种“合成工具”。</li>
<li>这些合成工具包含了各种接口定义、功能描述和潜在用途，极大地丰富了模型对“工具”概念的泛化能力，使其能更好地理解和适应未知工具。</li>
</ul>
</li>
<li>
<p><strong>构建庞大的“合成 Agent 场景”</strong></p>
<ul>
<li>通过组合不同的系统提示（System Prompt）和不同类型的工具集，Moonshot AI 模拟了数千个不同的“代理”（Agent）角色和应用场景。</li>
<li>这为模型提供了在各种复杂情境下进行决策的训练环境。</li>
</ul>
</li>
<li>
<p><strong>模拟用户-代理交互轨迹</strong></p>
<ul>
<li>在上述模拟的代理环境下，Moonshot AI 进行了大量的“用户-代理”多轮交互模拟。这个过程记录了 Agent 的完整“思考 → 选择工具 → 调用 → 观察工具结果 → 继续推理”的轨迹。</li>
<li>这些轨迹的一部分是基于真实工具执行环境（例如数学计算、编程环境）生成的，另一部分则是完全合成的，旨在探索更广阔的决策空间。</li>
</ul>
</li>
<li>
<p><strong>评估与筛选轨迹，用于模型训练</strong> - 收集到的海量轨迹并非全部用于训练。Moonshot AI 利用 LLM 对这些轨迹进行评估，识别并剔除其中质量低劣、逻辑不合理或效果不佳的轨迹。 - 最终筛选出的高质量轨迹，与少量真实执行数据一起，用于对 Kimi K2 模型进行<strong>后训练（Post-Training）</strong>。这种训练的目的是在模型内部强化其对工具使用的决策能力和多步规划能力。
<img alt="this is a image" loading="lazy" src="/images/kimi_k2_tool_use.png">
Kimi K2 在这些后训练之后展现出令人瞩目的成果：</p>
</li>
</ol>
<ul>
<li><strong>卓越的工具使用能力</strong>：Kimi K2 在多个工具使用基准测试中表现卓越。在实际产品应用（如复杂的编辑差异替换任务）中，其失败率低至约 3.3%，显示出极高的可靠性。</li>
<li><strong>高性价比的性能</strong>：尽管其运行成本低于一些市场领先的模型，但在“工具使用”这一关键能力上，Kimi K2 能够匹敌甚至超越部分高端模型，展现了后训练的巨大潜力。</li>
<li><strong>“工具接口能力”成为新的 UX 层面</strong>：这表明后训练的价值远不止于“对话能力增强”。它使得“工具接口能力”本身成为模型与用户互动的新界面，改变了用户与 AI 协作的体验。</li>
<li><strong>强调“使用策略”的重要性</strong>：Kimi K2 的成功再次证明，仅仅提供工具是不够的。模型需要从训练中习得“在什么场景下用钻头”、“如何安全有效地操作工具”的策略和判断力。</li>
</ul>
<p>Kimi K2 的实践揭示了 AI 工具能力的深层突破：它通过大规模真实与合成工具、场景及交互轨迹的后训练，成功地将“会用工具的策略和智慧”内化进模型核心，让工具执行能力成为模型固有的组成部分，而不再仅仅是外部挂载的模块。</p>
<p>未来的 AI，不仅是“大脑”，更是“手”——既能推理，又能行动，让智能不止于思考，而是能够真正执行、操作并落地到现实世界。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Thinking in Agents</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
