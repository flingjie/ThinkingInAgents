<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Thinking in Agents</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Thinking in Agents</description>
    <generator>Hugo -- 0.151.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 15:12:29 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>上下文工程之DeepAgents</title>
      <link>http://localhost:1313/posts/context_engineering_in_deepagents/</link>
      <pubDate>Thu, 13 Nov 2025 15:12:29 +0800</pubDate>
      <guid>http://localhost:1313/posts/context_engineering_in_deepagents/</guid>
      <description></description>
    </item>
    <item>
      <title>Agent开发之上下文工程</title>
      <link>http://localhost:1313/posts/context_engineering/</link>
      <pubDate>Wed, 12 Nov 2025 16:21:12 +0800</pubDate>
      <guid>http://localhost:1313/posts/context_engineering/</guid>
      <description>&lt;p&gt;随着 AI 系统和多 Agent 架构的发展，“上下文工程”这一概念逐渐成为设计智能体系统的核心话题。&lt;/p&gt;
&lt;p&gt;Manus 团队在其博客文章《AI 代理的上下文工程：构建 Manus 的经验教训》中提到，在打造智能体系统的过程中，他们面临一个根本抉择：是从头训练一个端到端的智能代理模型，还是基于已有的大语言模型（LLM）构建 “上下文学习”能力？
正是对“上下文”这一维度的深入反思，让他们最终押注于“上下文工程”（Context Engineering）。&lt;/p&gt;
&lt;p&gt;在当下，智能体系统越来越被要求实现多轮交互、状态追踪、环境变化的感知与响应。缺乏对上下文的系统化管理，就意味着智能体容易“丢链子”：记不住上一轮、误判当前状态、无法在复杂任务中持续推进。Manus 的经验清楚地告诉我们：模型能力虽强，若缺乏结构化的上下文策略，其潜力也会大打折扣。&lt;/p&gt;
&lt;p&gt;因此，本文将围绕 “上下文工程” 这一核心展开：从其定义到发展历程，再到一个实践示例，最终回顾关键要点并探讨未来趋势。希望你在搭建智能体系统时，能少走几步、快步收敛。&lt;/p&gt;
&lt;h2 id=&#34;什么是上下文工程context-engineering&#34;&gt;&lt;strong&gt;什么是上下文工程（Context Engineering）&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;提示词工程告诉模型“怎么问、怎么答”，而上下文工程则让模型 &lt;strong&gt;记住并利用过去的信息&lt;/strong&gt; ；前者关注单次输入，后者关注多轮连续性，两者互为补充。&lt;/p&gt;
&lt;p&gt;可以把它想象成和朋友聊天的场景：你们聊了半个小时，你的朋友记得你之前说过的话，知道你今天的计划，也了解你的喜好。当你问“今晚吃什么”时，他就能结合之前的聊天内容，给出更贴心、更合理的建议。&lt;/p&gt;
&lt;p&gt;上下文工程让智能体也能像这个朋友一样——&lt;strong&gt;不仅“听懂”当前输入，还能“记住”历史信息并合理利用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;会记住历史信息（你说过什么、做过什么）&lt;/li&gt;
&lt;li&gt;会结合当前任务进行判断&lt;/li&gt;
&lt;li&gt;可以管理不同任务或不同用户的上下文，避免混乱&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简而言之，上下文工程就是让智能体&lt;strong&gt;有记忆、有判断力&lt;/strong&gt;，不仅看现在，更懂过去和环境。&lt;/p&gt;
&lt;h2 id=&#34;从注意力机制到上下文工程&#34;&gt;&lt;strong&gt;从注意力机制到上下文工程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;要理解上下文工程的本质，可以从大语言模型的核心架构——&lt;strong&gt;Transformer&lt;/strong&gt; 说起。
Transformer 的关键思想来自那篇里程碑式论文《Attention Is All You Need》。
自注意力机制（Self-Attention）让模型在处理每个词时，都能动态地“关注”输入序列中与之最相关的部分。模型不再是线性阅读，而是在整个上下文中分配“注意力”，由此形成理解。&lt;/p&gt;
&lt;p&gt;从这个角度看，&lt;strong&gt;上下文工程其实是系统层面的注意力管理机制&lt;/strong&gt;。
Transformer 的 attention 决定模型在输入序列内部该关注哪些 token；
而上下文工程决定模型在更高层次上该关注哪些&lt;strong&gt;历史、任务、环境或工具&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;换句话说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer 的注意力是“微观”的，聚焦在词与词之间；&lt;/li&gt;
&lt;li&gt;上下文工程的注意力是“宏观”的，聚焦在对话、记忆、任务和工具之间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在复杂的智能体系统中，这种“外部注意力”尤为关键。
它帮助系统判断：什么时候该引用历史，什么时候该忽略噪声，什么时候聚焦当下。
如果说 Transformer 是&lt;strong&gt;微观的注意力调度器&lt;/strong&gt;，那么上下文工程就是&lt;strong&gt;宏观的注意力编排师&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;发展历程&#34;&gt;&lt;strong&gt;发展历程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GAIR-NLP 将上下文工程定位为一个涉及&lt;strong&gt;上下文收集、存储、管理和利用&lt;/strong&gt;的工程过程，目标是缩小人类意图与机器理解之间的差距，并提出了上下文工程的四个发展阶段。&lt;/p&gt;
&lt;h3 id=&#34;四阶段演进模型&#34;&gt;&lt;strong&gt;四阶段演进模型&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;时代 1.0&lt;/strong&gt;：原始计算阶段，只能处理结构化输入，对上下文处理能力有限。人机交互成本较高，因为机器只能接受非常明确的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 2.0&lt;/strong&gt;：以智能体为中心，出现自然语言处理、大语言模型、多模态处理和模糊信息处理能力。机器智能提升，人机交互成本下降，可以容忍更高熵（更杂乱、非结构化）的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 3.0（未来）&lt;/strong&gt;：类人智能阶段，智能体开始理解和推理更复杂的上下文关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 4.0（未来设想）&lt;/strong&gt;：超人智能阶段，智能体能够处理极高复杂度和不确定性的上下文，实现超人级决策和理解能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;this is a image&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/context_overview.png&#34;&gt;
GAIR-NLP 指出，当前主流大语言模型（如 GPT-3 及其后续版本）具备以下 2.0 特征：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent development — Think in patterns, not frameworks</title>
      <link>http://localhost:1313/posts/agent_development/</link>
      <pubDate>Tue, 11 Nov 2025 17:38:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/agent_development/</guid>
      <description>&lt;h2 id=&#34;1-why-off-the-shelf-frameworks-are-starting-to-fail&#34;&gt;1. Why “off-the-shelf frameworks” are starting to fail&lt;/h2&gt;
&lt;p&gt;A framework is a tool for imposing order. It helps you set boundaries amid messy requirements, makes collaboration predictable, and lets you reproduce results.&lt;/p&gt;
&lt;p&gt;Whether it’s a business framework (OKR) or a technical framework (React, LangChain), its value is that it makes experience portable and complexity manageable.&lt;/p&gt;
&lt;p&gt;But frameworks assume a stable problem space and well-defined goals. The moment your system operates in a high-velocity, high-uncertainty environment, that advantage falls apart:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a Customer Support Agent: From Linear Flows to Expert Routing</title>
      <link>http://localhost:1313/posts/building_a_customer_support_agent/</link>
      <pubDate>Tue, 11 Nov 2025 17:03:14 +0800</pubDate>
      <guid>http://localhost:1313/posts/building_a_customer_support_agent/</guid>
      <description>&lt;p&gt;Traditional customer service bots rely heavily on &lt;em&gt;if/else&lt;/em&gt; rules and rigid intent-matching. The moment a user says something vague or deviates from the expected flow, the system breaks down. This is what we call &lt;strong&gt;“process thinking.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the Agent era, we shift toward &lt;strong&gt;“strategic thinking”&lt;/strong&gt; — building intelligent systems that can make decisions autonomously and dynamically route conversations to the right experts. Such a system isn’t just an LLM; it’s an &lt;strong&gt;LLM-powered network of specialized experts.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>My View on Agents: From Workflows to Strategic Thinking</title>
      <link>http://localhost:1313/posts/my-view-on-agents/</link>
      <pubDate>Tue, 11 Nov 2025 14:24:50 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-view-on-agents/</guid>
      <description>&lt;p&gt;OpenAI defines an &lt;em&gt;Agent&lt;/em&gt; as a system that integrates model capabilities, tool interfaces, and strategies — capable of autonomously perceiving, deciding, acting, and improving its performance.&lt;/p&gt;
&lt;p&gt;Claude, on the other hand, highlights the goal-driven and interactive nature of Agents: they not only understand and generate information, but also refine their behavior through continuous feedback.&lt;/p&gt;
&lt;p&gt;In my view, if an LLM is the &lt;strong&gt;brain&lt;/strong&gt;, then an Agent is the &lt;strong&gt;body that acts on behalf of that brain&lt;/strong&gt;.
An LLM is like a &lt;em&gt;super-intelligent search engine and content generator&lt;/em&gt; — it can understand problems and produce answers, but it doesn’t act on its own.
An Agent, in contrast, is like a &lt;em&gt;thoughtful, hands-on assistant&lt;/em&gt; — it not only understands and generates, but also takes initiative and adapts based on feedback.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
