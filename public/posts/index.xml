<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Thinking in Agents</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Thinking in Agents</description>
    <generator>Hugo -- 0.151.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 09:46:48 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>一次Agent项目失利手记</title>
      <link>http://localhost:1313/posts/a_failed_agent_project/</link>
      <pubDate>Tue, 02 Dec 2025 09:46:48 +0800</pubDate>
      <guid>http://localhost:1313/posts/a_failed_agent_project/</guid>
      <description>&lt;p&gt;今年上半年，我接手了一个让我心潮澎湃的项目：为一家业务飞速扩张的公司构建一套内部智能 Agent 系统。我们的愿景很宏大——希望 Agent 能自主处理部分工单、高效汇总业务信息，甚至智能触发内部流程。那时候，AI 圈子正热得发烫，人人都在谈 Agent，我也觉得自己站在了风口浪尖。&lt;/p&gt;
&lt;p&gt;然而，现实却狠狠地泼了一盆冷水。项目非但没有跑出预期的曲线，反而像一连串的“技术地雷”，接二连三地爆炸。&lt;/p&gt;
&lt;p&gt;直到项目被紧急叫停的那一刻，我才恍然大悟：许多看似纯粹的技术难题，实则根植于我们对工程化理念的忽视。&lt;/p&gt;
&lt;p&gt;这篇文字，是我对那段经历的一次痛彻心扉的回顾。&lt;/p&gt;
&lt;h3 id=&#34;一项目开局一切顺利得让人不安&#34;&gt;&lt;strong&gt;一、项目开局：一切顺利得让人不安&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;说实话，项目的启动阶段顺利得有些过分。&lt;/p&gt;
&lt;p&gt;我们迅速选用了当时最新的大模型，搭建了一个简洁的对话界面，并初步绑定了几项核心业务工具。仅仅两周时间，一个像模像样的 Demo 就成功跑通了几个典型的工单处理流程。&lt;/p&gt;
&lt;p&gt;领导和客户看了 Demo，赞不绝口。那一刻，我心头升起一股傲气，觉得凭我们团队的能力，这次 Agent 落地定能一帆风顺。&lt;/p&gt;
&lt;p&gt;然而，正是这种盲目的乐观，为后来的急转直下埋下了伏笔。当我们将系统接入真实的业务数据环境时，好戏才真正开场。&lt;/p&gt;
&lt;h3 id=&#34;二第一次撞墙agent-在信息洪流中迷失了方向&#34;&gt;&lt;strong&gt;二、第一次“撞墙”：Agent 在信息洪流中迷失了方向&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;灰度上线的第一天，系统刚运行没几个小时，我们后台的监控告警就开始疯狂闪烁。&lt;/p&gt;
&lt;p&gt;原因听起来有些啼笑皆非：Agent 本应根据用户输入的关键词，准确判断工单是售后问题还是物流咨询。但在某些特定场景下，它会错误地将工单识别为“未知类型”，随即陷入一个“怪圈”——反复调用同一个查询工具，频率之高，一度让我们以为系统正遭受 DDoS 攻击。&lt;/p&gt;
&lt;p&gt;那天晚上，我盯着日志，看到怀疑人生：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;调用：工单查询 → 发现没结果 → 尝试改写问题 → 再次工单查询 → 仍然没结果 → 又一次改写&amp;hellip;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;它就像一个被困在房间里的机器人，固执地、一遍又一遍地敲击着同一扇根本打不开的门。我们后来形象地称之为“围墙式死循环”。&lt;/p&gt;
&lt;h3 id=&#34;三愈演愈烈agent-开始胡乱发号施令&#34;&gt;&lt;strong&gt;三、愈演愈烈：Agent 开始“胡乱发号施令”&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;进入第二周，一个更为棘手的事故发生了。&lt;/p&gt;
&lt;p&gt;在某些复杂的边缘场景下，Agent 再次做出了错误的选择，这次它误选了一个用于触发“审批提醒”的工具，导致两个毫不相关的部门突然收到了莫名其妙的审批通知。&lt;/p&gt;
&lt;p&gt;谢天谢地，这只是一个低风险的提醒流程，没有造成真正的生产事故，但却在公司内部引起了不小的混乱。客户那边来电话质询时，我脑子里的第一反应不是恐慌，而是茫然：“怎么又是 Agent？我们不是刚修好了那个死循环吗？”&lt;/p&gt;
&lt;p&gt;深入排查后，我们才摸清了症结：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;工具命名风格混乱：&lt;/strong&gt; 缺乏统一的规范。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;返回数据结构不一致：&lt;/strong&gt; 各个工具的输出五花八门。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调用约定模糊：&lt;/strong&gt; 不同的工具调用方式差异大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当工具数量从最初的 8 个迅速膨胀到 20 多个时，Agent 的内部决策逻辑彻底陷入了混乱。那一刻我才醍醐灌顶：&lt;strong&gt;我们根本没有建立起一套健全的“工具治理体系”！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;回想起来，将几十个未经规范和约束的工具一股脑地扔给一个 AI 智能体自由调用，这和放任一个不了解业务的实习生随意操作上百个内部 API 几乎没有区别——甚至可能更加危险。&lt;/p&gt;
&lt;h3 id=&#34;四团队的疲惫信心开始动摇&#34;&gt;&lt;strong&gt;四、团队的疲惫：信心开始动摇&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;有一段时间，我们团队几乎每天都在为新的边缘案例焦头烂额，而且越到后期，修补的难度越大。&lt;/p&gt;
&lt;p&gt;前端同事抱怨 Agent 的行为模式难以预测，导致界面交互频繁出错。后端同事则深陷于错综复杂的调用日志，感觉像在迷宫里探索。而最直接的业务方，客户，也反馈体验越来越“怪异”。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI 的未来，不在大脑，而在手：从 LLM 到工具驱动智能</title>
      <link>http://localhost:1313/posts/teaching_llm_to_use_tools/</link>
      <pubDate>Thu, 20 Nov 2025 13:59:13 +0800</pubDate>
      <guid>http://localhost:1313/posts/teaching_llm_to_use_tools/</guid>
      <description>&lt;p&gt;在 AI 领域，我们常陷入一个误区：认为智能的未来在于构建&lt;strong&gt;更大、更全知、更像人脑&lt;/strong&gt;的模型，让它记住所有知识。然而，作为开发者，我们深知这种“记忆力”的上限和局限。人类的真正智能，并非依赖于单纯的知识存储，而在于&lt;strong&gt;解决问题的能力&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;思考我们自己解决一个复杂数学题的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;分析问题&lt;/strong&gt;：理解需求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择工具&lt;/strong&gt;：判断需要哪些数学工具、公式或软件。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建模与执行&lt;/strong&gt;：运用工具进行计算、推导。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;验证与决策&lt;/strong&gt;：检查结果，修正思路，得出最终结论。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这个过程中，最核心的不是你“背诵”了多少公式，而是你&lt;strong&gt;知道何时何地调用何种工具，并能高效利用它们&lt;/strong&gt;。这是一种以“结构化思维 + 目标导向”为核心的模式。&lt;/p&gt;
&lt;h3 id=&#34;llm-的本质与局限语言的概率压缩器&#34;&gt;LLM 的本质与局限：语言的概率压缩器&lt;/h3&gt;
&lt;p&gt;从开发者角度看，当前的大型语言模型（LLM）本质是一个&lt;strong&gt;强大的语言概率压缩器&lt;/strong&gt;。它在海量文本数据中学习词语间的统计关联，从而能生成极为流利、上下文相关的文本。对于纯语言任务，其表现令人惊叹。&lt;/p&gt;
&lt;p&gt;然而，当我们尝试让 LLM 驾驭&lt;strong&gt;非语言、强逻辑、精确性&lt;/strong&gt;的领域时，如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;抽象符号运算&lt;/strong&gt;（数学、物理）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精确定义匹配&lt;/strong&gt;（工程规范）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;严格逻辑推导&lt;/strong&gt;（编程、决策树）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可验证的计算过程&lt;/strong&gt;（财务、科学实验）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们会发现语言模型固有的“概率化”机制，与这些领域要求的“确定性”之间存在着&lt;strong&gt;先天鸿沟&lt;/strong&gt;。指望 LLM 仅仅靠扩大规模来跨越这条鸿沟，就像期望一个语言学家通过背诵更多诗歌就能成为量子物理学家一样，这是不现实且低效的。所以，需要借助工具扩展 LLM 的能力。&lt;/p&gt;
&lt;h2 id=&#34;一扩展-llm-能力借用人类使用工具的智慧&#34;&gt;一、扩展 LLM 能力：借用人类“使用工具”的智慧&lt;/h2&gt;
&lt;p&gt;人类的进步正是通过外部工具不断&lt;strong&gt;突破自身生理和认知极限&lt;/strong&gt;：计算器扩展了计算力，笔记扩展了记忆，显微镜扩展了感知，电脑扩展了逻辑运算能力。我们的大脑并非用于无限计算，而是用于&lt;strong&gt;调度工具、规划步骤、制定策略和验证结果。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因此，作为开发者，我们不应止步于“训练更大模型”，而应专注于赋予 LLM &lt;strong&gt;“使用工具”的能力&lt;/strong&gt;。当前 AI 领域已清晰地展现了这一趋势和技术价值：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;解决 LLM 算术硬伤：集成计算器工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：即使是参数巨大的 LLM，在两位数以上的精确计算上依然错误频发，因为其内部知识是概率模式，而非符号计算引擎。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：通过&lt;strong&gt;将外部计算器作为工具&lt;/strong&gt;接入。LLM 识别出计算任务后，不再尝试内部计算，而是将数字和运算符传递给计算器工具执行，再将准确结果整合到回复中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;价值&lt;/strong&gt;：外部计算器提供了 LLM 内部无法比拟的精确执行能力，极大地提升了模型在数值任务上的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;克服 LLM 信息“幻觉”：引入搜索引擎与知识库工具&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：LLM 训练数据的时效性、全面性限制，以及生成文本时为了流畅性可能出现的“编造”现象，导致其信息准确性难以保证。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：引入&lt;strong&gt;搜索引擎或外部知识库查询工具&lt;/strong&gt;。LLM 在生成答案前，先调用这些工具检索最新、最准确的事实依据。例如，回答“最新诺贝尔奖得主”时，模型通过工具获取实时数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;价值&lt;/strong&gt;：外部工具弥补了 LLM 内部知识的盲区与时效性问题，提供了“事实校准”和“实时更新”的能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;突破 LLM 逻辑推理瓶颈：集成代码解释器与符号系统&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：LLM 在处理多步复杂逻辑推理、编程调试或科学问题时，往往难以保持推理链条的连贯和正确。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;代码解释器（Code Interpreter）&lt;/strong&gt;：LLM 生成 Python 等代码，然后将代码交给外部解释器运行验证。模型可根据运行结果（包括错误信息）进行调试和迭代。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;符号推理系统&lt;/strong&gt;：对于严谨的数学或逻辑问题，LLM 将问题转化为符号表达式，交由专门的工具处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;价值&lt;/strong&gt;：这些工具为 LLM 提供了其内部缺乏的精确执行和验证逻辑的能力，是其逻辑推理的强大“外挂”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些实践案例清晰地表明，在 AI 发展的关键阶段，尤其是面对需要&lt;strong&gt;超越语言模式、进行精确操作、获取实时信息和严格逻辑验证&lt;/strong&gt;的场景时，&lt;strong&gt;仅仅依赖模型内部的“知识”是远远不够的。&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Context Prompt-Driven Self-Learning: The Future of Agent Cognition</title>
      <link>http://localhost:1313/posts/context_prompt_driven_self_learning/</link>
      <pubDate>Sat, 15 Nov 2025 01:42:11 +0800</pubDate>
      <guid>http://localhost:1313/posts/context_prompt_driven_self_learning/</guid>
      <description>&lt;p&gt;In the world of artificial intelligence, we&amp;rsquo;re moving beyond simply feeding agents with information and telling them what to do. &lt;strong&gt;Agents shouldn’t just run on context — they should learn from it&lt;/strong&gt;. The context window isn’t just a temporary storage for prompts; it&amp;rsquo;s a dynamic medium for &lt;strong&gt;meta-learning&lt;/strong&gt;. It’s not just a scaffold, but the &lt;strong&gt;substrate&lt;/strong&gt; on which the agent&amp;rsquo;s intelligence thrives.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s rethink what the context in a prompt actually means. Imagine the prompt as more than just a one-off instruction. Instead, it’s a &lt;em&gt;portable cognitive environment&lt;/em&gt; where the agent constantly remembers, reasons, reflects, and refines itself — continuously improving its performance. This is the core idea behind &lt;strong&gt;context-prompt-driven self-learning&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent开发之上下文工程:让智能体“持续地思考”</title>
      <link>http://localhost:1313/posts/context_engineering/</link>
      <pubDate>Wed, 12 Nov 2025 16:21:12 +0800</pubDate>
      <guid>http://localhost:1313/posts/context_engineering/</guid>
      <description>&lt;p&gt;随着 AI 系统和多 Agent 架构的发展，“上下文工程”这一概念逐渐成为设计智能体系统的核心话题。&lt;/p&gt;
&lt;p&gt;Manus 团队在其博客文章《AI 代理的上下文工程：构建 Manus 的经验教训》中提到，在打造智能体系统的过程中，他们面临一个根本抉择：是从头训练一个端到端的智能代理模型，还是基于已有的大语言模型（LLM）构建 “上下文学习”能力？
正是对“上下文”这一维度的深入反思，让他们最终押注于“上下文工程”（Context Engineering）。&lt;/p&gt;
&lt;p&gt;在当下，智能体系统越来越被要求实现多轮交互、状态追踪、环境变化的感知与响应。缺乏对上下文的系统化管理，就意味着智能体容易“丢链子”：记不住上一轮、误判当前状态、无法在复杂任务中持续推进。Manus 的经验清楚地告诉我们：模型能力虽强，若缺乏结构化的上下文策略，其潜力也会大打折扣。&lt;/p&gt;
&lt;p&gt;因此，本文将围绕 “上下文工程” 这一核心展开：从其定义到发展历程，再到一个实践示例，最终回顾关键要点并探讨未来趋势。希望你在搭建智能体系统时，能少走几步、快步收敛。&lt;/p&gt;
&lt;h2 id=&#34;什么是上下文工程context-engineering&#34;&gt;&lt;strong&gt;什么是上下文工程（Context Engineering）&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;提示词工程告诉模型“怎么问、怎么答”，而上下文工程则让模型 &lt;strong&gt;记住并利用过去的信息&lt;/strong&gt; ；前者关注单次输入，后者关注多轮连续性，两者互为补充。&lt;/p&gt;
&lt;p&gt;可以把它想象成和朋友聊天的场景：你们聊了半个小时，你的朋友记得你之前说过的话，知道你今天的计划，也了解你的喜好。当你问“今晚吃什么”时，他就能结合之前的聊天内容，给出更贴心、更合理的建议。&lt;/p&gt;
&lt;p&gt;上下文工程让智能体也能像这个朋友一样——&lt;strong&gt;不仅“听懂”当前输入，还能“记住”历史信息并合理利用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;会记住历史信息（你说过什么、做过什么）&lt;/li&gt;
&lt;li&gt;会结合当前任务进行判断&lt;/li&gt;
&lt;li&gt;可以管理不同任务或不同用户的上下文，避免混乱&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简而言之，上下文工程就是让智能体&lt;strong&gt;有记忆、有判断力&lt;/strong&gt;，不仅看现在，更懂过去和环境。&lt;/p&gt;
&lt;h2 id=&#34;从注意力机制到上下文工程&#34;&gt;&lt;strong&gt;从注意力机制到上下文工程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;要理解上下文工程的本质，可以从大语言模型的核心架构——&lt;strong&gt;Transformer&lt;/strong&gt; 说起。
Transformer 的关键思想来自那篇里程碑式论文《Attention Is All You Need》。
自注意力机制（Self-Attention）让模型在处理每个词时，都能动态地“关注”输入序列中与之最相关的部分。模型不再是线性阅读，而是在整个上下文中分配“注意力”，由此形成理解。&lt;/p&gt;
&lt;p&gt;从这个角度看，&lt;strong&gt;上下文工程其实是系统层面的注意力管理机制&lt;/strong&gt;。
Transformer 的 attention 决定模型在输入序列内部该关注哪些 token；
而上下文工程决定模型在更高层次上该关注哪些&lt;strong&gt;历史、任务、环境或工具&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;换句话说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer 的注意力是“微观”的，聚焦在词与词之间；&lt;/li&gt;
&lt;li&gt;上下文工程的注意力是“宏观”的，聚焦在对话、记忆、任务和工具之间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在复杂的智能体系统中，这种“外部注意力”尤为关键。
它帮助系统判断：什么时候该引用历史，什么时候该忽略噪声，什么时候聚焦当下。
如果说 Transformer 是&lt;strong&gt;微观的注意力调度器&lt;/strong&gt;，那么上下文工程就是&lt;strong&gt;宏观的注意力编排师&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;发展历程&#34;&gt;&lt;strong&gt;发展历程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GAIR-NLP 将上下文工程定位为一个涉及&lt;strong&gt;上下文收集、存储、管理和利用&lt;/strong&gt;的工程过程，目标是缩小人类意图与机器理解之间的差距，并提出了上下文工程的四个发展阶段。&lt;/p&gt;
&lt;h3 id=&#34;四阶段演进模型&#34;&gt;&lt;strong&gt;四阶段演进模型&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;时代 1.0&lt;/strong&gt;：原始计算阶段，只能处理结构化输入，对上下文处理能力有限。人机交互成本较高，因为机器只能接受非常明确的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 2.0&lt;/strong&gt;：以智能体为中心，出现自然语言处理、大语言模型、多模态处理和模糊信息处理能力。机器智能提升，人机交互成本下降，可以容忍更高熵（更杂乱、非结构化）的上下文信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 3.0（未来）&lt;/strong&gt;：类人智能阶段，智能体开始理解和推理更复杂的上下文关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时代 4.0（未来设想）&lt;/strong&gt;：超人智能阶段，智能体能够处理极高复杂度和不确定性的上下文，实现超人级决策和理解能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;this is a image&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/context_overview.png&#34;&gt;
GAIR-NLP 指出，当前主流大语言模型（如 GPT-3 及其后续版本）具备以下 2.0 特征：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent development — Think in patterns, not frameworks</title>
      <link>http://localhost:1313/posts/agent_development/</link>
      <pubDate>Tue, 11 Nov 2025 17:38:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/agent_development/</guid>
      <description>&lt;h2 id=&#34;1-why-off-the-shelf-frameworks-are-starting-to-fail&#34;&gt;1. Why “off-the-shelf frameworks” are starting to fail&lt;/h2&gt;
&lt;p&gt;A framework is a tool for imposing order. It helps you set boundaries amid messy requirements, makes collaboration predictable, and lets you reproduce results.&lt;/p&gt;
&lt;p&gt;Whether it’s a business framework (OKR) or a technical framework (React, LangChain), its value is that it makes experience portable and complexity manageable.&lt;/p&gt;
&lt;p&gt;But frameworks assume a stable problem space and well-defined goals. The moment your system operates in a high-velocity, high-uncertainty environment, that advantage falls apart:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a Customer Support Agent: From Linear Flows to Expert Routing</title>
      <link>http://localhost:1313/posts/building_a_customer_support_agent/</link>
      <pubDate>Tue, 11 Nov 2025 17:03:14 +0800</pubDate>
      <guid>http://localhost:1313/posts/building_a_customer_support_agent/</guid>
      <description>&lt;p&gt;Traditional customer service bots rely heavily on &lt;em&gt;if/else&lt;/em&gt; rules and rigid intent-matching. The moment a user says something vague or deviates from the expected flow, the system breaks down. This is what we call &lt;strong&gt;“process thinking.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the Agent era, we shift toward &lt;strong&gt;“strategic thinking”&lt;/strong&gt; — building intelligent systems that can make decisions autonomously and dynamically route conversations to the right experts. Such a system isn’t just an LLM; it’s an &lt;strong&gt;LLM-powered network of specialized experts.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>My View on Agents: From Workflows to Strategic Thinking</title>
      <link>http://localhost:1313/posts/my-view-on-agents/</link>
      <pubDate>Tue, 11 Nov 2025 14:24:50 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-view-on-agents/</guid>
      <description>&lt;p&gt;OpenAI defines an &lt;em&gt;Agent&lt;/em&gt; as a system that integrates model capabilities, tool interfaces, and strategies — capable of autonomously perceiving, deciding, acting, and improving its performance.&lt;/p&gt;
&lt;p&gt;Claude, on the other hand, highlights the goal-driven and interactive nature of Agents: they not only understand and generate information, but also refine their behavior through continuous feedback.&lt;/p&gt;
&lt;p&gt;In my view, if an LLM is the &lt;strong&gt;brain&lt;/strong&gt;, then an Agent is the &lt;strong&gt;body that acts on behalf of that brain&lt;/strong&gt;.
An LLM is like a &lt;em&gt;super-intelligent search engine and content generator&lt;/em&gt; — it can understand problems and produce answers, but it doesn’t act on its own.
An Agent, in contrast, is like a &lt;em&gt;thoughtful, hands-on assistant&lt;/em&gt; — it not only understands and generates, but also takes initiative and adapts based on feedback.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
