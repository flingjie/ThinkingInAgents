<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Agent开发之上下文工程:让智能体“持续地思考” | Thinking in Agents</title>
<meta name="keywords" content="">
<meta name="description" content="随着 AI 系统和多 Agent 架构的发展，“上下文工程”这一概念逐渐成为设计智能体系统的核心话题。
Manus 团队在其博客文章《AI 代理的上下文工程：构建 Manus 的经验教训》中提到，在打造智能体系统的过程中，他们面临一个根本抉择：是从头训练一个端到端的智能代理模型，还是基于已有的大语言模型（LLM）构建 “上下文学习”能力？
正是对“上下文”这一维度的深入反思，让他们最终押注于“上下文工程”（Context Engineering）。
在当下，智能体系统越来越被要求实现多轮交互、状态追踪、环境变化的感知与响应。缺乏对上下文的系统化管理，就意味着智能体容易“丢链子”：记不住上一轮、误判当前状态、无法在复杂任务中持续推进。Manus 的经验清楚地告诉我们：模型能力虽强，若缺乏结构化的上下文策略，其潜力也会大打折扣。
因此，本文将围绕 “上下文工程” 这一核心展开：从其定义到发展历程，再到一个实践示例，最终回顾关键要点并探讨未来趋势。希望你在搭建智能体系统时，能少走几步、快步收敛。
什么是上下文工程（Context Engineering）
提示词工程告诉模型“怎么问、怎么答”，而上下文工程则让模型 记住并利用过去的信息 ；前者关注单次输入，后者关注多轮连续性，两者互为补充。
可以把它想象成和朋友聊天的场景：你们聊了半个小时，你的朋友记得你之前说过的话，知道你今天的计划，也了解你的喜好。当你问“今晚吃什么”时，他就能结合之前的聊天内容，给出更贴心、更合理的建议。
上下文工程让智能体也能像这个朋友一样——不仅“听懂”当前输入，还能“记住”历史信息并合理利用：

会记住历史信息（你说过什么、做过什么）
会结合当前任务进行判断
可以管理不同任务或不同用户的上下文，避免混乱

简而言之，上下文工程就是让智能体有记忆、有判断力，不仅看现在，更懂过去和环境。
从注意力机制到上下文工程
要理解上下文工程的本质，可以从大语言模型的核心架构——Transformer 说起。
Transformer 的关键思想来自那篇里程碑式论文《Attention Is All You Need》。
自注意力机制（Self-Attention）让模型在处理每个词时，都能动态地“关注”输入序列中与之最相关的部分。模型不再是线性阅读，而是在整个上下文中分配“注意力”，由此形成理解。
从这个角度看，上下文工程其实是系统层面的注意力管理机制。
Transformer 的 attention 决定模型在输入序列内部该关注哪些 token；
而上下文工程决定模型在更高层次上该关注哪些历史、任务、环境或工具。
换句话说：

Transformer 的注意力是“微观”的，聚焦在词与词之间；
上下文工程的注意力是“宏观”的，聚焦在对话、记忆、任务和工具之间。

在复杂的智能体系统中，这种“外部注意力”尤为关键。
它帮助系统判断：什么时候该引用历史，什么时候该忽略噪声，什么时候聚焦当下。
如果说 Transformer 是微观的注意力调度器，那么上下文工程就是宏观的注意力编排师。
发展历程
GAIR-NLP 将上下文工程定位为一个涉及上下文收集、存储、管理和利用的工程过程，目标是缩小人类意图与机器理解之间的差距，并提出了上下文工程的四个发展阶段。
四阶段演进模型

时代 1.0：原始计算阶段，只能处理结构化输入，对上下文处理能力有限。人机交互成本较高，因为机器只能接受非常明确的上下文信息。
时代 2.0：以智能体为中心，出现自然语言处理、大语言模型、多模态处理和模糊信息处理能力。机器智能提升，人机交互成本下降，可以容忍更高熵（更杂乱、非结构化）的上下文信息。
时代 3.0（未来）：类人智能阶段，智能体开始理解和推理更复杂的上下文关系。
时代 4.0（未来设想）：超人智能阶段，智能体能够处理极高复杂度和不确定性的上下文，实现超人级决策和理解能力。


GAIR-NLP 指出，当前主流大语言模型（如 GPT-3 及其后续版本）具备以下 2.0 特征：">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/context_engineering/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/context_engineering/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Thinking in Agents (Alt + H)">Thinking in Agents</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Agent开发之上下文工程:让智能体“持续地思考”
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-11-12 16:21:12 +0800 CST'>November 12, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>随着 AI 系统和多 Agent 架构的发展，“上下文工程”这一概念逐渐成为设计智能体系统的核心话题。</p>
<p>Manus 团队在其博客文章《AI 代理的上下文工程：构建 Manus 的经验教训》中提到，在打造智能体系统的过程中，他们面临一个根本抉择：是从头训练一个端到端的智能代理模型，还是基于已有的大语言模型（LLM）构建 “上下文学习”能力？
正是对“上下文”这一维度的深入反思，让他们最终押注于“上下文工程”（Context Engineering）。</p>
<p>在当下，智能体系统越来越被要求实现多轮交互、状态追踪、环境变化的感知与响应。缺乏对上下文的系统化管理，就意味着智能体容易“丢链子”：记不住上一轮、误判当前状态、无法在复杂任务中持续推进。Manus 的经验清楚地告诉我们：模型能力虽强，若缺乏结构化的上下文策略，其潜力也会大打折扣。</p>
<p>因此，本文将围绕 “上下文工程” 这一核心展开：从其定义到发展历程，再到一个实践示例，最终回顾关键要点并探讨未来趋势。希望你在搭建智能体系统时，能少走几步、快步收敛。</p>
<h2 id="什么是上下文工程context-engineering"><strong>什么是上下文工程（Context Engineering）</strong><a hidden class="anchor" aria-hidden="true" href="#什么是上下文工程context-engineering">#</a></h2>
<p>提示词工程告诉模型“怎么问、怎么答”，而上下文工程则让模型 <strong>记住并利用过去的信息</strong> ；前者关注单次输入，后者关注多轮连续性，两者互为补充。</p>
<p>可以把它想象成和朋友聊天的场景：你们聊了半个小时，你的朋友记得你之前说过的话，知道你今天的计划，也了解你的喜好。当你问“今晚吃什么”时，他就能结合之前的聊天内容，给出更贴心、更合理的建议。</p>
<p>上下文工程让智能体也能像这个朋友一样——<strong>不仅“听懂”当前输入，还能“记住”历史信息并合理利用</strong>：</p>
<ul>
<li>会记住历史信息（你说过什么、做过什么）</li>
<li>会结合当前任务进行判断</li>
<li>可以管理不同任务或不同用户的上下文，避免混乱</li>
</ul>
<p>简而言之，上下文工程就是让智能体<strong>有记忆、有判断力</strong>，不仅看现在，更懂过去和环境。</p>
<h2 id="从注意力机制到上下文工程"><strong>从注意力机制到上下文工程</strong><a hidden class="anchor" aria-hidden="true" href="#从注意力机制到上下文工程">#</a></h2>
<p>要理解上下文工程的本质，可以从大语言模型的核心架构——<strong>Transformer</strong> 说起。
Transformer 的关键思想来自那篇里程碑式论文《Attention Is All You Need》。
自注意力机制（Self-Attention）让模型在处理每个词时，都能动态地“关注”输入序列中与之最相关的部分。模型不再是线性阅读，而是在整个上下文中分配“注意力”，由此形成理解。</p>
<p>从这个角度看，<strong>上下文工程其实是系统层面的注意力管理机制</strong>。
Transformer 的 attention 决定模型在输入序列内部该关注哪些 token；
而上下文工程决定模型在更高层次上该关注哪些<strong>历史、任务、环境或工具</strong>。</p>
<p>换句话说：</p>
<ul>
<li>Transformer 的注意力是“微观”的，聚焦在词与词之间；</li>
<li>上下文工程的注意力是“宏观”的，聚焦在对话、记忆、任务和工具之间。</li>
</ul>
<p>在复杂的智能体系统中，这种“外部注意力”尤为关键。
它帮助系统判断：什么时候该引用历史，什么时候该忽略噪声，什么时候聚焦当下。
如果说 Transformer 是<strong>微观的注意力调度器</strong>，那么上下文工程就是<strong>宏观的注意力编排师</strong>。</p>
<h2 id="发展历程"><strong>发展历程</strong><a hidden class="anchor" aria-hidden="true" href="#发展历程">#</a></h2>
<p>GAIR-NLP 将上下文工程定位为一个涉及<strong>上下文收集、存储、管理和利用</strong>的工程过程，目标是缩小人类意图与机器理解之间的差距，并提出了上下文工程的四个发展阶段。</p>
<h3 id="四阶段演进模型"><strong>四阶段演进模型</strong><a hidden class="anchor" aria-hidden="true" href="#四阶段演进模型">#</a></h3>
<ul>
<li><strong>时代 1.0</strong>：原始计算阶段，只能处理结构化输入，对上下文处理能力有限。人机交互成本较高，因为机器只能接受非常明确的上下文信息。</li>
<li><strong>时代 2.0</strong>：以智能体为中心，出现自然语言处理、大语言模型、多模态处理和模糊信息处理能力。机器智能提升，人机交互成本下降，可以容忍更高熵（更杂乱、非结构化）的上下文信息。</li>
<li><strong>时代 3.0（未来）</strong>：类人智能阶段，智能体开始理解和推理更复杂的上下文关系。</li>
<li><strong>时代 4.0（未来设想）</strong>：超人智能阶段，智能体能够处理极高复杂度和不确定性的上下文，实现超人级决策和理解能力。</li>
</ul>
<p><img alt="this is a image" loading="lazy" src="/images/context_overview.png">
GAIR-NLP 指出，当前主流大语言模型（如 GPT-3 及其后续版本）具备以下 2.0 特征：</p>
<ul>
<li>能处理非结构化、含糊的输入</li>
<li>能进行主动推理</li>
<li>上下文不仅是“输入信息”，更是“指令或指导”</li>
</ul>
<p>这些特征标志着我们已经进入上下文工程 2.0 阶段——智能体不仅能理解即时输入，还能利用上下文做出连续、智能的决策。</p>
<h2 id="实践与示例"><strong>实践与示例</strong><a hidden class="anchor" aria-hidden="true" href="#实践与示例">#</a></h2>
<p>下面的示例演示如何用 <strong>LangChain</strong> 构建一个简单的上下文工程系统，模拟“订机票”的多轮对话场景，让模型能够记住历史信息并结合当前需求给出合理回复。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.vectorstores <span style="color:#f92672">import</span> FAISS
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.embeddings.openai <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chat_models <span style="color:#f92672">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> ConversationalRetrievalChain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.memory <span style="color:#f92672">import</span> ConversationBufferMemory
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- 1. 初始化 LLM 和向量存储 ---</span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> OpenAIEmbeddings()
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> FAISS(embeddings<span style="color:#f92672">.</span>embed_query, dimension<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>)  <span style="color:#75715e"># 将文本转为向量</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- 2. 设置会话记忆 ---</span>
</span></span><span style="display:flex;"><span>memory <span style="color:#f92672">=</span> ConversationBufferMemory(memory_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;chat_history&#34;</span>, return_messages<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- 3. 创建 LLM 对话链 ---</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> ChatOpenAI(model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-4&#34;</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>)
</span></span><span style="display:flex;"><span>conversation_chain <span style="color:#f92672">=</span> ConversationalRetrievalChain<span style="color:#f92672">.</span>from_llm(
</span></span><span style="display:flex;"><span>    llm<span style="color:#f92672">=</span>llm,
</span></span><span style="display:flex;"><span>    retriever<span style="color:#f92672">=</span>vector_store<span style="color:#f92672">.</span>as_retriever(search_kwargs<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;k&#34;</span>: <span style="color:#ae81ff">5</span>}),  <span style="color:#75715e"># 检索相似历史信息</span>
</span></span><span style="display:flex;"><span>    memory<span style="color:#f92672">=</span>memory,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- 4. 模拟多轮对话 ---</span>
</span></span><span style="display:flex;"><span>user_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;user_a&#34;</span>
</span></span><span style="display:flex;"><span>user_inputs <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;我想订明天去北京的机票&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;帮我选最便宜的航班&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;希望上午出发&#34;</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> user_input <span style="color:#f92672">in</span> user_inputs:
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> conversation_chain<span style="color:#f92672">.</span>run(user_input)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;模型回复:&#34;</span>, response)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- 5. 新输入结合上下文生成回复 ---</span>
</span></span><span style="display:flex;"><span>latest_input <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;请帮我确认航班信息&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> conversation_chain<span style="color:#f92672">.</span>run(latest_input)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">最新回复:&#34;</span>, response)
</span></span></code></pre></div><hr>
<h3 id="生成回复过程详细解说"><strong>生成回复过程详细解说</strong><a hidden class="anchor" aria-hidden="true" href="#生成回复过程详细解说">#</a></h3>
<p>在上面的代码中，当你输入一条信息时，系统生成回复的流程大致如下：</p>
<ol>
<li>
<p><strong>用户输入接收</strong></p>
<ul>
<li>用户发送请求（如“我想订明天去北京的机票”）。</li>
<li>系统将输入传递给 <code>ConversationalRetrievalChain</code>。</li>
</ul>
</li>
<li>
<p><strong>上下文检索</strong></p>
<ul>
<li><strong>Memory 检查历史对话</strong>：<code>ConversationBufferMemory</code> 会返回当前用户之前的对话内容。</li>
<li><strong>向量检索（FAISS）</strong>：将用户输入转为向量，并检索与之最相似的历史上下文（k=5 条最相关信息）。</li>
<li>检索出的上下文会作为附加信息输入给 LLM。</li>
</ul>
</li>
<li>
<p><strong>生成提示（Prompt Construction）</strong></p>
<ul>
<li>
<p><code>ConversationalRetrievalChain</code> 会将当前输入 + 历史上下文 + 检索到的相似信息拼接成一个完整的提示（prompt）。</p>
</li>
<li>
<p>例如：</p>
<pre tabindex="0"><code>历史对话：
用户：我想订明天去北京的机票
系统：好的，请问希望哪家航空公司？

当前输入：
帮我选最便宜的航班

请根据以上信息给出建议。
</code></pre></li>
</ul>
</li>
<li>
<p><strong>模型生成回复</strong></p>
<ul>
<li>LLM 接收到完整提示后，通过自身的注意力机制（attention）整合历史上下文和当前输入，生成符合上下文的回答。</li>
<li>此时模型“记得”用户的之前需求，也能考虑新的约束条件。</li>
</ul>
</li>
<li>
<p><strong>更新记忆</strong></p>
<ul>
<li>系统将这轮对话加入 <code>ConversationBufferMemory</code>。</li>
<li>下一轮对话时，模型可以直接访问这条记录，从而实现连续、多轮对话。</li>
</ul>
</li>
<li>
<p><strong>返回给用户</strong></p>
<ul>
<li>系统输出生成的文本，作为最终回复。</li>
<li>如果用户输入进一步要求（如“请帮我确认航班信息”），流程重复，但上下文会随着对话累积而不断丰富，生成的回复会更加精准和个性化。</li>
</ul>
</li>
</ol>
<h3 id="流程优势"><strong>流程优势</strong><a hidden class="anchor" aria-hidden="true" href="#流程优势">#</a></h3>
<ul>
<li><strong>多轮连续性</strong>：模型不仅处理当前输入，还能结合历史对话。</li>
<li><strong>信息回溯</strong>：通过向量检索，跨会话也能调用相关历史信息。</li>
<li><strong>个性化</strong>：结合用户偏好和历史行为生成更贴近需求的回答。</li>
<li><strong>易扩展</strong>：可以支持多用户、多任务甚至跨 Agent 的上下文管理。</li>
</ul>
<h2 id="总结与展望"><strong>总结与展望</strong><a hidden class="anchor" aria-hidden="true" href="#总结与展望">#</a></h2>
<p>本文从定义、发展、实践示例到案例分析，全面梳理了上下文工程在智能体系统中的应用。核心要点包括：</p>
<ul>
<li>上下文工程不仅是状态管理，而是智能化、任务导向的上下文管理</li>
<li>动态和智能上下文管理是未来趋势</li>
<li>实践中需注意上下文膨胀、隔离及可扩展性</li>
</ul>
<p>未来，随着 LLM 和多 Agent 系统的发展，上下文工程将成为智能体系统设计的核心能力。通过合理设计和优化上下文策略，智能体能够更精准、更连贯地理解和执行任务，从而实现更高水平的智能交互。</p>
<h2 id="参考资料">参考资料<a hidden class="anchor" aria-hidden="true" href="#参考资料">#</a></h2>
<ul>
<li><a href="https://manus.im/zh-cn/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">AI 代理的上下文工程：构建 Manus 的经验教训</a></li>
<li><a href="https://github.com/GAIR-NLP/Context-Engineering-2.0">Context Engineering 2.0</a></li>
<li><a href="https://github.com/langchain-ai/deepagents">Deepagents</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Thinking in Agents</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
