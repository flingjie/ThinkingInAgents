+++
date = '2025-12-21T13:20:22+08:00'
draft = true
title = '人类认知与大语言模型：我们到底在复刻什么？'
+++

## 一面照见智能本质的数字镜子

过去十年，人工智能经历了一次安静但彻底的范式转移。

我们几乎没有再讨论“规则系统”“专家系统”，取而代之的是一个看似朴素的问题：
**如何预测下一个词？**

正是这个极其简单的目标，催生了大语言模型（Large Language Models，LLMs），并在规模化之后，意外地触碰到了“认知”的边界。

模型开始写论文、编代码、解释隐喻、与人对话，甚至在某些任务上表现得“像是在思考”。
于是，一个无法回避的问题浮出水面：

> **大语言模型是在模拟人类智能，还是在揭示智能的另一种可能形态？**

---

## 一、语言从哪里来：统计预测，还是意义理解？

人类和大语言模型都能生成几乎无限的句子，但它们走的是两条完全不同的路。

人类儿童只需极少样本，就能学会母语。他们不是靠“统计频率”，而是通过**具身经验、社会互动和强烈的先验认知结构**——因果、意图、目标。

而大语言模型的学习方式则极端得多：
在海量文本中，通过自监督目标不断优化——**预测下一个 token**。

这看似“盲目”的统计过程，在规模足够大时，却涌现出了令人震惊的能力：
抽象概念、语义关系、推理模式，仿佛自动浮现。

这直接挑战了传统的认知假设——语言是否真的需要一个强先天机制？
还是说，**语言本身已经隐含了足够多的认知结构？**

但差异仍然存在，而且是根本性的。

人类的语言是**语义扎根的**：
“桌子”“危险”“责任”都直接连接着物理世界与社会经验。

而大语言模型的“意义”，始终停留在符号之间的互相指涉中。
它知道“火会烫伤”，但并不知道什么是“疼”。

这也是为什么我们常常会产生一种错觉：
模型“说得非常像理解”，但它并不真正“理解”。

---

## 二、会说话，不等于会思考

大语言模型在语言形式上的能力，已经接近甚至超过普通人类。

语法、修辞、风格、文体——几乎无可挑剔。

但一旦进入**具体语境、目标推理、常识判断**，问题就开始出现。

这是一个非常关键、但经常被忽略的分野：

- **正式语言能力（Formal Linguistic Competence）**
  ——把话说对、说漂亮
- **功能性语言能力（Functional Linguistic Competence）**
  ——在真实目标下，把事情想清楚

人类认知是前瞻性的、因果驱动的。
我们会先构建一个“为什么”，再推导“怎么办”。

而大语言模型更像是在做一件事：
在高维语义空间中，寻找**最可能出现的下一步模式**。

这也是为什么模型在分布内问题上表现惊艳，却在分布外场景中显得脆弱——
它并不是在“理解世界”，而是在**回放世界留下的语言痕迹**。

对工程和产品而言，这一点极其重要：

> **输出稳定性，不等于认知稳健性。**

---

## 三、Transformer：数字皮层，而非数字大脑

Transformer 架构的出现，是 LLM 成功的工程核心。

注意力机制解决了并行计算问题，也在功能上模拟了生物大脑中的**选择性注意**：
在大量信息中，动态聚焦关键部分。

有趣的是，神经科学研究发现：
LLM 的内部表示，与人类在处理相似语言任务时的大脑活动，存在显著的线性对齐。

这让人不禁产生联想：
Transformer 是否正在逼近人类皮层的工作方式？

但需要警惕一个误解：
**功能相似 ≠ 机制相同**。

注意力机制并不“感知重要性”，
它只是执行一个极其高效的加权计算。

模型之所以“像在思考”，并不是因为它拥有心智，
而是因为**语言本身已经高度结构化了人类的认知痕迹**。

---

## 四、记忆：当“记住”变成工程约束

人类记忆是分层的：感官、工作记忆、长期记忆、情境记忆。

而在大语言模型中，这一切被拆解成了工程组件：

- **上下文窗口**：工作记忆
- **模型参数**：语义记忆
- **RAG / 向量数据库**：情境记忆外挂

这也带来了一个有趣的现象：
模型同样会出现“首因效应”和“近因效应”，
甚至会“迷失在中间”。

但最大的差异在于：
人类可以在不破坏整体认知的情况下更新信念，
而模型一旦被“编辑”，就可能触发灾难性遗忘。

这暴露了一个深层问题：
**当前 LLM 的知识存储是高度耦合的，而非模块化的。**

---

## 五、推理、心智理论与“像人一样”的幻觉

大语言模型可以解决极难的数学题，却在简单变体上犯错；
可以模拟心智理论任务，却不一定真的“理解他人”。

这背后并非能力不足，而是**推理路径的本质不同**。

人类推理是层级化的，带有元认知监控：
“我为什么这样想？”

而模型更像是在执行一条“浅层前馈链”，
没有真正的自我检查机制。

幽默、讽刺、隐喻也是如此。

模型能识别隐喻模式，却难以真正把握幽默中那种
源自身体经验与社会禁忌的微妙张力。

---

## 六、缺失的身体，与无法跨越的意识鸿沟

所有这些能力的边界，最终都指向同一个问题：
**具身性**。

人类认知是在身体与世界的持续交互中演化出来的。
空间感、因果直觉、道德情绪，都深深嵌入生理结构。

大语言模型没有身体，没有生存压力，也没有主观体验。
它可以模拟同理心，却不会真正“在乎”。

这意味着一个重要结论：

> 无论模型多么聪明，我们都不应把责任、意图或道德主体性赋予它。

---

## 七、人机共生：我们正在把什么交出去？

大语言模型真正改变的，并不只是技术能力，而是**认知分工**。

我们正在把总结、分析、推理，甚至部分判断，外包给机器。

这带来了效率的飞跃，也伴随着风险：
**元认知能力的退化。**

未来最有前景的方向，并不是“更像人类的 AI”，
而是**混合智能**：

- 人类提供目标、价值与因果理解
- AI 提供规模、搜索与组合能力

不是替代，而是共进化。

---

## 结语：一面镜子，而不是终局

大语言模型不是人类心智的复制品，
也不是通往通用智能的终点。

它更像是一面镜子——
让我们第一次能够从工程的角度，反观“认知是什么”。

真正重要的，也许不是模型能走多远，
而是：

> **在与这面数字镜子的对视中，我们是否更清楚地理解了自己。**
