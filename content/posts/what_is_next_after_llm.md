+++
date = '2025-12-20T21:42:55+08:00'
draft = true
title = '大模型之后是什么？从《哥德尔、巴赫、艾舍尔》看 AI 的下一步'
+++

如果你最近在做 AI 产品，可能会深陷于一种**“认知分裂”**：

- **Demo 越来越惊艳**：发布会上能力列表越拉越长，多步推理、复杂编程信手拈来。
- **业务频繁“翻车”**：一旦进入真实业务场景，逻辑断层、幻觉频发，在看似简单的问题上表现得像个“智力断崖”的巨婴。

模型看起来无所不知，却又不像真的懂。

一个终极拷问开始被反复提起：**大语言模型，究竟是在逼近“智能”的本质，还是仅仅把“概率拟合”玩到了极致？**

重读神作《哥德尔、巴赫、艾舍尔》（GEB）后，我产生了一些启发，与各位交流。

## 一、 我们得到了“能力”，但没有得到“理解”

从工程角度看，大模型的成就毋庸置疑：语言生成、代码补全、流程编排。但当你真正将其产品化时，会发现一个核心断层：

> **模型很会“说正确的话”，但它并不知道“为什么是对的”。**

它更像一个反应极快、表达力极强的系统，而非一个真正理解世界的智能体。一个自然的工程假设是：**“再堆一点算力，模型大到一定程度，会不会就‘自然懂了’？”**

GEB 给出的启示是：**不一定，而且很可能不会。** 因为理解不仅仅是信息的累加，更是结构的涌现。

## 二、 杨立昆的批评：文本智能的“天花板”

杨立昆（Yann LeCun）曾多次直言：现有模型本质上是在处理文本的“表面统计”，它们从未真正理解物理世界的因果律。

这指向了认知科学中经典的**符号接地问题（Symbol Grounding Problem）**：

- **大模型的“重力”**：源于数万亿 Token 的统计相关性。它知道“重力”常和“下落”出现在同一个句子。
- **五岁孩子的“重力”**：源于无数次看见物体掉落，以及自己摔跤时的切肤之痛。

这两种“理解”的来源完全不同。

## 三、 关键不在模型，而在数据形态

如果将不同智能系统的“输入”进行对比，你会发现 LLM 缺失了最关键的补丁：

| 数据类型               | 信息密度与学习方式               | 认知价值                           |
| ---------------------- | -------------------------------- | ---------------------------------- |
| **文本语料**           | 约 30 万亿 Token (阅读 50 万年)  | 掌握社会习俗、逻辑表达与显性知识   |
| **感官流 (视频/触觉)** | 四岁儿童积累约 100 万亿字节      | 构建物理常识、直觉物理学与空间意识 |
| **具身交互**           | 实时因果反馈 (Action → Response) | 习得操控物体、运动控制与因果推断   |

**文本数据的“聪明”，无法自动转化为对世界的“直觉”。** 这解释了为什么模型能写出优美的诗歌，却会在简单的物理常识（比如：把瓶子倒转，里面的水会怎样）上犯错。

## 四、 世界模型：从“语言连续”走向“因果连续”

这正是为什么“大模型之后”的共识，正在转向 **World Models（世界模型）**。

尤其是杨立昆提出的 **JEPA（联合嵌入预测架构）**。其核心思想是：**不再预测“下一个 Token”，而是预测“世界状态的演化”。**

这是从“语言智能”走向“环境智能”的关键一步。只有当模型能够预测“如果我这样做，世界会发生什么”时，它才开始拥有真正的理解。

## 五、 哥德尔不完备性：为什么系统需要“元认知”

GEB 的第一条主线来自哥德尔不完备性定理。将其工程化，可以得到一个结论：

> **任何足够复杂的系统，都存在它在系统内部无法证明的真理。**

映射到 AI 系统中：模型越强，覆盖的问题空间越大，它就越不可能仅靠内部概率来判断“自己是否在胡说”。

下一步的关键不是“更准”，而是**知道什么时候不确定**。这正是搜索增强、强化学习和推理校验（Reasoning）的真正价值——它们不是让模型更聪明，而是为系统引入了**元认知（Meta-cognition）**。

## 六、 神经符号智能：给“醉酒诗人”装上“刹车”

当前 AI 圈正在经历一场“神经符号复兴”。用 GEB 的隐喻来说：

- **LLM 像“醉酒诗人”**：联想丰富、生成迅速，但偶尔逻辑混乱。
- **未来的 AI 需要一位“严肃会计师”**：逻辑严谨、可校验、可解释。

这对应了丹尼尔·卡尼曼的框架：**系统 1（直觉/联想）与系统 2（逻辑/校验）**。今天的大模型是极致的系统 1，而神经符号方法本质上是在为 AI 引入系统 2。

## 七、 巴赫与艾舍尔：从“模式模仿”到“自我闭环”

巴赫的音乐告诉我们：创造力不来自规则的缺失，而来自**在极端约束下的结构构建**。

而艾舍尔的“怪圈”则揭示了“自我”的来源：当一个系统开始**描述自己、影响自己、并根据这种描述调整行为**时，真正的智能才会出现。

目前的大多数 AI 依然是“失忆”的：

- 没有稳定的自我边界。
- 没有连续的行为历史。
- 没有针对错误的自我修正机制。

## 八、 Agent

很多人会觉得：
Agent 不就是给 LLM 包一层流程吗？

但如果从 GEB 的视角看，Agent 的意义完全不同。

Agent 是第一次在工程层面，把三件事放在了一起：

1. **反思与不确定性**（哥德尔）
2. **结构化规划与组合**（巴赫）
3. **系统作用于自身**（艾舍尔）

这不是能力升级，而是**结构升级**。

## 结语

《哥德尔、巴赫、艾舍尔》帮我们看清了一件事：

**智能，从来不等于“可证明的正确性”。**

真正的智能系统，是明知自己不完备，却仍然能够行动；是在反馈中修正，在不确定中前进。大模型只是这趟长征的起点。**真正的挑战，是构建一个能“看见自己”的系统。**
